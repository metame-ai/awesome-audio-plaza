# Awesome VITS

- [Awesome VITS](#awesome-vits)
  - [Projects](#projects)
  - [Papers](#papers)
  - [Reference](#reference)


## Projects
- [**ttts**](https://github.com/adelacvg/ttts) - adelacvg ![Star](https://img.shields.io/github/stars/adelacvg/ttts.svg?style=social&label=Star)

	 *Train the next generation of TTS systems.*
- [**ar-vits**](https://github.com/innnky/ar-vits?tab=readme-ov-file) - innnky ![Star](https://img.shields.io/github/stars/innnky/ar-vits.svg?style=social&label=Star)

	 *text to speech using autoregressive transformer and VITS*

## Papers
- **Llama-VITS: Enhancing TTS Synthesis with Semantic Awareness**, `arXiv, 2404.06714`, [arxiv](http://arxiv.org/abs/2404.06714v2), [pdf](http://arxiv.org/pdf/2404.06714v2.pdf), cication: [**-1**](None)

	 *Xincan Feng, Akifumi Yoshimoto*
- **PAVITS: Exploring Prosody-aware VITS for End-to-End Emotional Voice
  Conversion**, `arXiv, 2403.01494`, [arxiv](http://arxiv.org/abs/2403.01494v1), [pdf](http://arxiv.org/pdf/2403.01494v1.pdf), cication: [**-1**](None)

	 *Tianhua Qi, Wenming Zheng, Cheng Lu, Yuan Zong, Hailun Lian* · ([jeremychee4.github](https://jeremychee4.github.io/pavits4EVC/))
- **VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech
  with Adversarial Learning and Architecture Design**, `arXiv, 2307.16430`, [arxiv](http://arxiv.org/abs/2307.16430v1), [pdf](http://arxiv.org/pdf/2307.16430v1.pdf), cication: [**-1**](None)

	 *Jungil Kong, Jihoon Park, Beomjeong Kim, Jeongmin Kim, Dohee Kong, Sangjin Kim*

    - [GitHub - p0p4k/vits2\_pytorch: unofficial vits2-TTS implementation in pytorch](https://github.com/p0p4k/vits2_pytorch)
    - [GitHub - daniilrobnikov/vits2: VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design](https://github.com/daniilrobnikov/vits2)
    - [Audio samples from "VITS2: Improving Quality and Efficiency of Single Stage Text to Speech with Adversarial Learning and Architecture Design"](https://vits-2.github.io/demo/)
    - [GitHub - fishaudio/Bert-VITS2: vits2 backbone with bert](https://github.com/fishaudio/Bert-VITS2)

- **NaturalSpeech: End-to-End Text to Speech Synthesis with Human-Level
  Quality**, `arXiv, 2205.04421`, [arxiv](http://arxiv.org/abs/2205.04421v2), [pdf](http://arxiv.org/pdf/2205.04421v2.pdf), cication: [**-1**](None)

	 *Xu Tan, Jiawei Chen, Haohe Liu, Jian Cong, Chen Zhang, Yanqing Liu, Xi Wang, Yichong Leng, Yuanhao Yi, Lei He*
    - [GitHub - heatz123/naturalspeech: A fully working pytorch implementation of NaturalSpeech (Tan et al., 2022)](https://github.com/heatz123/naturalspeech)
    - [NaturalSpeech: End-to-End Text to Speech Synthesis with Human-Level Quality - Speech Research](https://speechresearch.github.io/naturalspeech/)

- **Conditional Variational Autoencoder with Adversarial Learning for
  End-to-End Text-to-Speech**, `arXiv, 2106.06103`, [arxiv](http://arxiv.org/abs/2106.06103v1), [pdf](http://arxiv.org/pdf/2106.06103v1.pdf), cication: [**-1**](None)

	 *Jaehyeon Kim, Jungil Kong, Juhee Son*
    - VITS: [60、基于cVAE+Flow+GAN的效果最好语音合成VITS模型论文精讲\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1wU4y1q7po/?spm_id_from=333.337.search-card.all.click&vd_source=1453a06a1e0b377f5c40946333b4423a)
    - [Audio Samples from "Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech"](https://jaywalnut310.github.io/vits-demo/index.html)
    - [GitHub - jaywalnut310/vits: VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech](https://github.com/jaywalnut310/vits)

## Reference
- [**finetune-hf-vits**](https://github.com/ylacombe/finetune-hf-vits) - ylacombe ![Star](https://img.shields.io/github/stars/ylacombe/finetune-hf-vits.svg?style=social&label=Star)

	 *Finetune VITS and MMS using HuggingFace's tools*
- [**explore-vits**](https://huggingface.co/spaces/hf-audio/explore-vits) - hf-audio 🤗
- [举世无双语音合成系统 VITS 发展历程（2023.03.31 SNAC） - 知乎](https://zhuanlan.zhihu.com/p/474601997)