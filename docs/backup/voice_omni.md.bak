# Voice Omni

- [Voice Omni](#voice-omni) 
  - [Survey](#survey)
  - [Voice Omni](#voice-omni-1)
  - [Duplex](#duplex)
  - [Evaluation](#evaluation)
  - [Projects](#projects)
  - [Products](#products)
  - [Datasets](#datasets)
  - [Toolkits](#toolkits)
  - [Misc](#misc)


## Survey

- **WavChat: A Survey of Spoken Dialogue Models**, `arXiv, 2411.13577`, [arxiv](http://arxiv.org/abs/2411.13577v1), [pdf](http://arxiv.org/pdf/2411.13577v1.pdf), cication: [**-1**](None) 

	 *Shengpeng Ji, Yifu Chen, Minghui Fang, ..., Jin Xu, Zhou Zhao*
- [**Awesome-Speech-Language-Model**](https://github.com/ddlBoJack/Awesome-Speech-Language-Model) - ddlBoJack ![Star](https://img.shields.io/github/stars/ddlBoJack/Awesome-Speech-Language-Model.svg?style=social&label=Star) 
- **A Survey on Speech Large Language Models**, `arXiv, 2410.18908`, [arxiv](http://arxiv.org/abs/2410.18908v2), [pdf](http://arxiv.org/pdf/2410.18908v2.pdf), cication: [**-1**](None) 

	 *Jing Peng, Yucheng Wang, Yu Xi, ..., Xizhuo Zhang, Kai Yu*

## Voice Omni

- [Paper page - Kimi-Audio Technical Report](https://huggingface.co/papers/2504.18425)
   - [github.com](https://github.com/MoonshotAI/Kimi-Audio.)
- 🌟 **Qwen2.5-Omni Technical Report**, `arXiv, 2503.20215`, [arxiv](http://arxiv.org/abs/2503.20215v1), [pdf](http://arxiv.org/pdf/2503.20215v1.pdf), cication: [**-1**](None) 

	 *Jin Xu, Zhifang Guo, Jinzheng He, ..., Yunfei Chu, Junyang Lin* · ([chat.qwenlm](https://chat.qwenlm.ai)) · ([qwenlm.github](https://qwenlm.github.io/blog/qwen2.5-omni/)) · ([Qwen2.5-Omni](https://github.com/QwenLM/Qwen2.5-Omni) - QwenLM) ![Star](https://img.shields.io/github/stars/QwenLM/Qwen2.5-Omni.svg?style=social&label=Star) · ([huggingface](https://huggingface.co/Qwen/Qwen2.5-Omni-7B)) · ([huggingface](https://huggingface.co/spaces/Qwen/Qwen2.5-Omni-7B-Demo)) · ([modelscope](https://modelscope.cn/models/Qwen/Qwen2.5-Omni-7B)) · ([help.aliyun](https://help.aliyun.com/zh/model-studio/user-guide/qwen-omni)) · ([modelscope](https://modelscope.cn/studios/Qwen/Qwen2.5-Omni-Demo))
- **Vision-Speech Models: Teaching Speech Models to Converse about Images**, `arXiv, 2503.15633`, [arxiv](http://arxiv.org/abs/2503.15633v1), [pdf](http://arxiv.org/pdf/2503.15633v1.pdf), cication: [**-1**](None) 

	 *Amélie Royer, Moritz Böhle, Gabriel de Marmiesse, ..., Alexandre Défossez, Patrick Pérez* · ([kyutai](https://kyutai.org/moshivis)) · ([moshivis](https://github.com/kyutai-labs/moshivis) - kyutai-labs) ![Star](https://img.shields.io/github/stars/kyutai-labs/moshivis.svg?style=social&label=Star)
- **Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction**, `arXiv, 2502.17239`, [arxiv](http://arxiv.org/abs/2502.17239v1), [pdf](http://arxiv.org/pdf/2502.17239v1.pdf), cication: [**-1**](None) 

	 *Tianpeng Li, Jun Liu, Tao Zhang, ..., Zenan Zhou, Weipeng Chen* · ([baichuan-audio](https://github.com/baichuan-inc/baichuan-audio?tab=readme-ov-file) - baichuan-inc) ![Star](https://img.shields.io/github/stars/baichuan-inc/baichuan-audio.svg?style=social&label=Star)
- 🌟 **LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM**, `arXiv, 2503.04724`, [arxiv](http://arxiv.org/abs/2503.04724v1), [pdf](http://arxiv.org/pdf/2503.04724v1.pdf), cication: [**-1**](None) 

	 *Sambal Shikhar, Mohammed Irfan Kurpath, Sahal Shaji Mullappilly, ..., Salman Khan, Hisham Cholakkal* · ([LLMVoX](https://github.com/mbzuai-oryx/LLMVoX) - mbzuai-oryx) ![Star](https://img.shields.io/github/stars/mbzuai-oryx/LLMVoX.svg?style=social&label=Star)
- 🌟 [Phi-4-multimodal-instruct is a lightweight open multimodal foundationmodel that leverages the language, vision, and speech research](https://huggingface.co/microsoft/Phi-4-multimodal-instruct)  🤗 

	 · ([models](https://github.com/marketplace/models/azureml/Phi-4-multimodal-instruct/playground) - marketplace) ![Star](https://img.shields.io/github/stars/marketplace/models.svg?style=social&label=Star)
- **Baichuan-Omni-1.5 Technical Report**, `arXiv, 2501.15368`, [arxiv](http://arxiv.org/abs/2501.15368v1), [pdf](http://arxiv.org/pdf/2501.15368v1.pdf), cication: [**-1**](None) 

	 *Yadong Li, Jun Liu, Tao Zhang, ..., Zenan Zhou, Weipeng Chen* · ([Baichuan-Omni-1.5](https://github.com/baichuan-inc/Baichuan-Omni-1.5) - baichuan-inc) ![Star](https://img.shields.io/github/stars/baichuan-inc/Baichuan-Omni-1.5.svg?style=social&label=Star)
- **Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive 
  Modality Alignment**, `arXiv, 2502.04328`, [arxiv](http://arxiv.org/abs/2502.04328v2), [pdf](http://arxiv.org/pdf/2502.04328v2.pdf), cication: [**-1**](None) 

	 *Zuyan Liu, Yuhao Dong, Jiahui Wang, ..., Jiwen Lu, Yongming Rao* · ([ola-omni.github](https://ola-omni.github.io/)) · ([Ola](https://github.com/Ola-Omni/Ola) - Ola-Omni) ![Star](https://img.shields.io/github/stars/Ola-Omni/Ola.svg?style=social&label=Star) · ([huggingface](https://huggingface.co/THUdyh/Ola-7b))
- 🌟 **MinMo: A Multimodal Large Language Model for Seamless Voice Interaction**, `arXiv, 2501.06282`, [arxiv](http://arxiv.org/abs/2501.06282v1), [pdf](http://arxiv.org/pdf/2501.06282v1.pdf), cication: [**-1**](None) 

	 *Qian Chen, Yafeng Chen, Yanni Chen, ..., Chong Zhang, Jinren Zhou* · ([funaudiollm.github](https://funaudiollm.github.io/minmo/))
- [A GPT-4o Level MLLM for Vision, Speech and Multimodal Live Streaming on Your Phone](https://huggingface.co/openbmb/MiniCPM-o-2_6)  🤗 
- **OpenOmni: Large Language Models Pivot Zero-shot Omnimodal Alignment 
  across Language with Real-time Self-Aware Emotional Speech Synthesis**, `arXiv, 2501.04561`, [arxiv](http://arxiv.org/abs/2501.04561v2), [pdf](http://arxiv.org/pdf/2501.04561v2.pdf), cication: [**-1**](None) 

	 *Run Luo, Ting-En Lin, Haonan Zhang, ..., Hamid Alinejad-Rokny, Fei Huang*
- [GPT-4o](https://nonint.com/2024/05/14/gpt-4o/) 
- **VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction**, `arXiv, 2501.01957`, [arxiv](http://arxiv.org/abs/2501.01957v1), [pdf](http://arxiv.org/pdf/2501.01957v1.pdf), cication: [**-1**](None) 

	 *Chaoyou Fu, Haojia Lin, Xiong Wang, ..., Caifeng Shan, Ran He* · ([VITA](https://github.com/VITA-MLLM/VITA) - VITA-MLLM) ![Star](https://img.shields.io/github/stars/VITA-MLLM/VITA.svg?style=social&label=Star)
- **SLAM-Omni: Timbre-Controllable Voice Interaction System with 
  Single-Stage Training**, `arXiv, 2412.15649`, [arxiv](http://arxiv.org/abs/2412.15649v1), [pdf](http://arxiv.org/pdf/2412.15649v1.pdf), cication: [**-1**](None) 

	 *Wenxi Chen, Ziyang Ma, Ruiqi Yan, ..., Shujie Liu, Xie Chen* · ([slam-omni.github](https://slam-omni.github.io/))
- **Continuous Speech Tokens Makes LLMs Robust Multi-Modality Learners**, `arXiv, 2412.04917`, [arxiv](http://arxiv.org/abs/2412.04917v1), [pdf](http://arxiv.org/pdf/2412.04917v1.pdf), cication: [**-1**](None) 

	 *Ze Yuan, Yanqing Liu, Shujie Liu, ..., Sheng Zhao*
- **AlignFormer: Modality Matching Can Achieve Better Zero-shot 
  Instruction-Following Speech-LLM**, `arXiv, 2412.01145`, [arxiv](http://arxiv.org/abs/2412.01145v1), [pdf](http://arxiv.org/pdf/2412.01145v1.pdf), cication: [**-1**](None) 

	 *Ruchao Fan, Bo Ren, Yuxuan Hu, ..., Shujie Liu, Jinyu Li*
- 🌟 **InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for 
  Long-term Streaming Video and Audio Interactions**, `arXiv, 2412.09596`, [arxiv](http://arxiv.org/abs/2412.09596v1), [pdf](http://arxiv.org/pdf/2412.09596v1.pdf), cication: [**-1**](None) 

	 *Pan Zhang, Xiaoyi Dong, Yuhang Cao, ..., Dahua Lin, Jiaqi Wang* · ([InternLM-XComposer](https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-OmniLive) - InternLM) ![Star](https://img.shields.io/github/stars/InternLM/InternLM-XComposer.svg?style=social&label=Star) · ([huggingface](https://huggingface.co/internlm/internlm-xcomposer2d5-ol-7b))
- **Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition**, `arXiv, 2412.09501`, [arxiv](http://arxiv.org/abs/2412.09501v1), [pdf](http://arxiv.org/pdf/2412.09501v1.pdf), cication: [**-1**](None) 

	 *Zhisheng Zhong, Chengyao Wang, Yuqi Liu, ..., Shu Liu, Jiaya Jia* · ([lyra-omni.github](https://lyra-omni.github.io/)) · ([103.170.5](https://103.170.5.190:17860/)) · ([Lyra](https://github.com/dvlab-research/Lyra) - dvlab-research) ![Star](https://img.shields.io/github/stars/dvlab-research/Lyra.svg?style=social&label=Star) · ([huggingface](https://huggingface.co/collections/zszhong/lyra-model-674ea5bb3b39ff8f15de75fc))
- **Continuous Speech Tokens Makes LLMs Robust Multi-Modality Learners**, `arXiv, 2412.04917`, [arxiv](http://arxiv.org/abs/2412.04917v1), [pdf](http://arxiv.org/pdf/2412.04917v1.pdf), cication: [**-1**](None) 

	 *Ze Yuan, Yanqing Liu, Shujie Liu, ..., Sheng Zhao* · ([cognitivespeech.github](https://cognitivespeech.github.io/flowomni))
- **Advancing Speech Language Models by Scaling Supervised Fine-Tuning with 
  Over 60,000 Hours of Synthetic Speech Dialogue Data**, `arXiv, 2412.01078`, [arxiv](http://arxiv.org/abs/2412.01078v2), [pdf](http://arxiv.org/pdf/2412.01078v2.pdf), cication: [**-1**](None) 

	 *Shuaijiang Zhao, Tingwei Guo, Bajian Xiang, ..., Wei Zou, Xiangang Li* · ([huggingface](https://huggingface.co/spaces/KE-Team/KE-Omni))
- 🌟 [Paper page - Scaling Speech-Text Pre-training with Synthetic Interleaved Data](https://huggingface.co/papers/2411.17607) 
- **SALMONN-omni: A Codec-free LLM for Full-duplex Speech Understanding and 
  Generation**, `arXiv, 2411.18138`, [arxiv](http://arxiv.org/abs/2411.18138v1), [pdf](http://arxiv.org/pdf/2411.18138v1.pdf), cication: [**-1**](None) 

	 *Wenyi Yu, Siyin Wang, Xiaoyu Yang, ..., Yuxuan Wang, Chao Zhang*
- **Internalizing ASR with Implicit Chain of Thought for Efficient 
  Speech-to-Speech Conversational LLM**, `arXiv, 2409.17353`, [arxiv](http://arxiv.org/abs/2409.17353v3), [pdf](http://arxiv.org/pdf/2409.17353v3.pdf), cication: [**-1**](None) 

	 *Robin Shing-Hei Yuen, Timothy Tin-Long Tse, Jian Zhu*
- 🌟 **Building a Taiwanese Mandarin Spoken Language Model: A First Attempt**, `arXiv, 2411.07111`, [arxiv](http://arxiv.org/abs/2411.07111v1), [pdf](http://arxiv.org/pdf/2411.07111v1.pdf), cication: [**-1**](None) 

	 *Chih-Kai Yang, Yu-Kuan Fu, Chen-An Li, ..., Shu-wen Yang, Hung-yi Lee*
- [freddyaboulton / llama-code-editor](https://huggingface.co/spaces/freddyaboulton/llama-code-editor/tree/main)  🤗 
- **Align-SLM: Textless Spoken Language Models with Reinforcement Learning 
  from AI Feedback**, `arXiv, 2411.01834`, [arxiv](http://arxiv.org/abs/2411.01834v1), [pdf](http://arxiv.org/pdf/2411.01834v1.pdf), cication: [**-1**](None) 

	 *Guan-Ting Lin, Prashanth Gurunath Shivakumar, Aditya Gourav, ..., Hung-yi Lee, Ivan Bulyko*
- [Introducing hertz-dev, the first open-source base model for conversational audio generation](https://si.inc/hertz-dev/) 

	 · ([x](https://x.com/si_pbc/status/1853184307063660723)) · ([hertz-dev](https://github.com/Standard-Intelligence/hertz-dev?tab=readme-ov-file) - Standard-Intelligence) ![Star](https://img.shields.io/github/stars/Standard-Intelligence/hertz-dev.svg?style=social&label=Star)
- 🌟 **Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model 
  with Frozen LLM**, `arXiv, 2411.00774`, [arxiv](http://arxiv.org/abs/2411.00774v1), [pdf](http://arxiv.org/pdf/2411.00774v1.pdf), cication: [**-1**](None) 

	 *Xiong Wang, Yangze Li, Chaoyou Fu, ..., Xing Sun, Long Ma*

	 · ([freeze-omni.github](https://freeze-omni.github.io/))

	 · ([Freeze-Omni](https://github.com/VITA-MLLM/Freeze-Omni) - VITA-MLLM) ![Star](https://img.shields.io/github/stars/VITA-MLLM/Freeze-Omni.svg?style=social&label=Star)
- **Generative Expressive Conversational Speech Synthesis**, `arXiv, 2407.21491`, [arxiv](http://arxiv.org/abs/2407.21491v2), [pdf](http://arxiv.org/pdf/2407.21491v2.pdf), cication: [**-1**](None) 

	 *Rui Liu, Yifan Hu, Yi Ren, ..., Xiang Yin, Haizhou Li* · ([GPT-Talker](https://github.com/walker-hyf/GPT-Talker?tab=readme-ov-file) - walker-hyf) ![Star](https://img.shields.io/github/stars/walker-hyf/GPT-Talker.svg?style=social&label=Star) · ([mp.weixin.qq](https://mp.weixin.qq.com/s/bF4qnMrxcDSbVYHi4jGF_A))
- **Get Large Language Models Ready to Speak: A Late-fusion Approach for 
  Speech Generation**, `arXiv, 2410.20336`, [arxiv](http://arxiv.org/abs/2410.20336v1), [pdf](http://arxiv.org/pdf/2410.20336v1.pdf), cication: [**-1**](None)

	 *Maohao Shen, Shun Zhang, Jilong Wu, ..., Mike Seltzer, Qing He* · ([maohaos2.github](https://maohaos2.github.io/TTS-Llama-MoLE-Llama/))
- **GPT-4o System Card**, `arXiv, 2410.21276`, [arxiv](http://arxiv.org/abs/2410.21276v1), [pdf](http://arxiv.org/pdf/2410.21276v1.pdf), cication: [**-1**](None) 

	 *OpenAI, :, Aaron Hurst, ..., Yunxing Dai, Yury Malkov*
- **Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant**, `arXiv, 2410.15316`, [arxiv](http://arxiv.org/abs/2410.15316v1), [pdf](http://arxiv.org/pdf/2410.15316v1.pdf), cication: [**-1**](None) 

	 *Alan Dao, Dinh Bach Vu, Huy Hoang Ha* · ([ichigo.homebrew](https://ichigo.homebrew.ltd/)) · ([homebrew](https://homebrew.ltd/)) · ([ichigo](https://github.com/homebrewltd/ichigo) - homebrewltd) ![Star](https://img.shields.io/github/stars/homebrewltd/ichigo.svg?style=social&label=Star)
- 🌟 **Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex 
  Capabilities**, `arXiv, 2410.11190`, [arxiv](http://arxiv.org/abs/2410.11190v2), [pdf](http://arxiv.org/pdf/2410.11190v2.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=14534896134025731094&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Zhifei Xie, Changqiao Wu* · ([mini-omni2](https://github.com/gpt-omni/mini-omni2) - gpt-omni) ![Star](https://img.shields.io/github/stars/gpt-omni/mini-omni2.svg?style=social&label=Star)

## Duplex

- 🌟 **OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation**, `arXiv, 2410.17799`, [arxiv](http://arxiv.org/abs/2410.17799v1), [pdf](http://arxiv.org/pdf/2410.17799v1.pdf), cication: [**-1**](None) 

	 *Qinglin Zhang, Luyao Cheng, Chong Deng, ..., Hai Yu, Chaohong Tan*

	 · ([omniflatten.github](https://omniflatten.github.io/))
- **Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue 
  Agents**, `arXiv, 2409.15594`, [arxiv](http://arxiv.org/abs/2409.15594v1), [pdf](http://arxiv.org/pdf/2409.15594v1.pdf), cication: [**-1**](None)

	 *Bandhav Veluri, Benjamin N Peloquin, Bokai Yu, ..., Hongyu Gong, Shyamnath Gollakota*

	 · ([syncllm.cs.washington](https://syncllm.cs.washington.edu/))

## Evaluation

- **S2S-Arena, Evaluating Speech2Speech Protocols on Instruction Following 
  with Paralinguistic Information**, `arXiv, 2503.05085`, [arxiv](http://arxiv.org/abs/2503.05085v1), [pdf](http://arxiv.org/pdf/2503.05085v1.pdf), cication: [**-1**](None) 

	 *Feng Jiang, Zhiyu Lin, Fan Bu, ..., Benyou Wang, Haizhou Li*
- [Evaluating Audio Reasoning with Big Bench Audio](https://huggingface.co/blog/big-bench-audio-release)  🤗 
- **VoiceBench: Benchmarking LLM-Based Voice Assistants**, `arXiv, 2410.17196`, [arxiv](http://arxiv.org/abs/2410.17196v1), [pdf](http://arxiv.org/pdf/2410.17196v1.pdf), cication: [**-1**](None) 

	 *Yiming Chen, Xianghu Yue, Chen Zhang, ..., Robby T. Tan, Haizhou Li*

	 · ([VoiceBench](https://github.com/MatthewCYM/VoiceBench) - MatthewCYM) ![Star](https://img.shields.io/github/stars/MatthewCYM/VoiceBench.svg?style=social&label=Star)
- **Can Large Audio-Language Models Truly Hear? Tackling Hallucinations with 
  Multi-Task Assessment and Stepwise Audio Reasoning**, `arXiv, 2410.16130`, [arxiv](http://arxiv.org/abs/2410.16130v1), [pdf](http://arxiv.org/pdf/2410.16130v1.pdf), cication: [**-1**](None)

	 *Chun-Yi Kuan, Hung-yi Lee*

## Projects

- 🌟 [**Step-Audio**](https://github.com/stepfun-ai/Step-Audio) - stepfun-ai ![Star](https://img.shields.io/github/stars/stepfun-ai/Step-Audio.svg?style=social&label=Star) 

	 · ([Step-Audio](https://github.com/stepfun-ai/Step-Audio/blob/main/assets/Step-Audio.pdf) - stepfun-ai) ![Star](https://img.shields.io/github/stars/stepfun-ai/Step-Audio.svg?style=social&label=Star)
- 🌟 [**SpeechGPT-2.0-preview**](https://github.com/OpenMOSS/SpeechGPT-2.0-preview) - OpenMOSS ![Star](https://img.shields.io/github/stars/OpenMOSS/SpeechGPT-2.0-preview.svg?style=social&label=Star) 

	 · ([𝕏](https://x.com/dongzha35524835/status/1883721464639344984))
- 🌟 [**MiniCPM-o**](https://github.com/OpenBMB/MiniCPM-o) - OpenBMB ![Star](https://img.shields.io/github/stars/OpenBMB/MiniCPM-o.svg?style=social&label=Star) 
- [**openai-realtime-embedded-sdk**](https://github.com/openai/openai-realtime-embedded-sdk) - openai ![Star](https://img.shields.io/github/stars/openai/openai-realtime-embedded-sdk.svg?style=social&label=Star) 
- [**Infini-Megrez**](https://github.com/infinigence/Infini-Megrez) - infinigence ![Star](https://img.shields.io/github/stars/infinigence/Infini-Megrez.svg?style=social&label=Star) 
- [**AnyModal**](https://github.com/ritabratamaiti/AnyModal) - ritabratamaiti ![Star](https://img.shields.io/github/stars/ritabratamaiti/AnyModal.svg?style=social&label=Star) 

	 *A Flexible Multimodal Language Model Framework*
- [**VideoChat**](https://github.com/Henry-23/VideoChat) - Henry-23 ![Star](https://img.shields.io/github/stars/Henry-23/VideoChat.svg?style=social&label=Star) 
- [**ultravox**](https://github.com/fixie-ai/ultravox) - fixie-ai ![Star](https://img.shields.io/github/stars/fixie-ai/ultravox.svg?style=social&label=Star) 

	 · ([demo.ultravox](https://demo.ultravox.ai/)) · ([huggingface](https://huggingface.co/fixie-ai/))
- [**WavChat**](https://github.com/jishengpeng/WavChat) - jishengpeng ![Star](https://img.shields.io/github/stars/jishengpeng/WavChat.svg?style=social&label=Star) 

	 *A Survey of Spoken Dialogue Models*
- [**gradio-groq-basics**](https://github.com/bklieger-groq/gradio-groq-basics/blob/main/calorie-tracker/README.md) - bklieger-groq ![Star](https://img.shields.io/github/stars/bklieger-groq/gradio-groq-basics.svg?style=social&label=Star) 

	 · ([𝕏](https://x.com/BenjaminKlieger/status/1854266346290434501))
- [Fish Agent V0.1 3B is a groundbreaking Voice-to-Voice model capable of capturing and generating environmental audio information with unprecedented accuracy.](https://huggingface.co/fishaudio/fish-agent-v0.1-3b)  🤗 
- [KingNish / OpenGPT-4o](https://huggingface.co/spaces/KingNish/OpenGPT-4o/tree/main)  🤗 
- [**GLM-4-Voice**](https://github.com/THUDM/GLM-4-Voice) - THUDM ![Star](https://img.shields.io/github/stars/THUDM/GLM-4-Voice.svg?style=social&label=Star) 
- [homebrewltd / mini-Ichigo-llama3.2-3B-s-instruct](https://huggingface.co/homebrewltd/mini-Ichigo-llama3.2-3B-s-instruct)  🤗 

## Products

- [Perplexity Voice Mode](https://x.com/AravSrinivas/status/1894792092284789173)  𝕏 
- [Introducing OCTAVE, a next-generation speech-language model.](https://x.com/hume_ai/status/1871263932742246513)  𝕏 
- [Project Astra: Exploring a Universal AI Assistant with Greg Wayne](https://www.youtube.com/watch?v=ctWfv4WUp2I)  :clapper: 
- [voice AI hackathon](https://x.com/hingeloss/status/1851260286415593487)  𝕏 

## Datasets


## Toolkits


## Misc

- [OpenAI Realtime API: The Missing Manual](https://www.latent.space/p/realtime-api) 
- [声网刘斌：“Her”真正落地实现离不开RTE能力的支撑｜MEET 2025](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247769213&idx=2&sn=2fa5bd98d8e3eb4246a5ee74a5cd8595&chksm=e94b63006fa12a765db0fa3bd06422be070c414bf06795c9a10af729909a7bccef52872e05f6&scene=0&xtrack=1) 
- [OmniAudio is the world's fastest and most efficient audio-language model for on-device deployment](https://huggingface.co/NexaAIDev/OmniAudio-2.6B)  🤗 
- [**TEN-Agent**](https://github.com/TEN-framework/TEN-Agent) - TEN-framework ![Star](https://img.shields.io/github/stars/TEN-framework/TEN-Agent.svg?style=social&label=Star) 
- [Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming](https://www.semanticscholar.org/paper/Mini-Omni%3A-Language-Models-Can-Hear%2C-Talk-While-in-Xie-Wu/8625ab06f76caf36ab138bac20a3116ba5b298e6) 
- [Talk to AI with natural speech detection](https://x.com/BenjaminKlieger/status/1853899938561917355)  𝕏 
- [chatgpt voice demo with mini-omni 2 (multimodal)](https://x.com/freddy_alfonso_/status/1852071457406259553)  𝕏 
- [Mini-Omni 2 understands image, audio and text inputs all via end-to-end voice conversations with users](https://x.com/reach_vb/status/1850895844167286859)  𝕏 
- :clapper: [dotAI 2024 - Neil Zeghidour - Multimodal language models](https://www.youtube.com/watch?v=UgpLM9gNkqs) 
- [**s2s_endpoint**](https://huggingface.co/blog/s2s_endpoint) -  🤗 