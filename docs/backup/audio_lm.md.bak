# Audio Language Model

- [Audio Language Model](#audio-language-model) 
  - [Papers](#papers)
  - [Survey](#survey)
  - [Evaluation](#evaluation)
  - [Projects](#projects)
  - [Toolkits](#toolkits)
  - [Misc](#misc)


## Papers

- **AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large
  Language Models**, `arXiv, 2505.16211`, [arxiv](http://arxiv.org/abs/2505.16211v1), [pdf](http://arxiv.org/pdf/2505.16211v1.pdf), cication: [**-1**](None) 

	 *Kai Li, Can Shen, Yile Liu, ..., Wei Dong, Xinfeng Li* · ([AudioTrust.](https://github.com/JusperLee/AudioTrust.) - JusperLee) ![Star](https://img.shields.io/github/stars/JusperLee/AudioTrust..svg?style=social&label=Star)
- **Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?**, `arXiv, 2505.09439`, [arxiv](http://arxiv.org/abs/2505.09439v1), [pdf](http://arxiv.org/pdf/2505.09439v1.pdf), cication: [**-1**](None) 

	 *Andrew Rouditchenko, Saurabhchand Bhati, Edson Araujo, ..., Rogerio Feris, James Glass* · ([𝕏](https://x.com/arouditchenko/status/1922977115856576610))
- **SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction 
  Fine-Tuning**, `arXiv, 2504.09081`, [arxiv](http://arxiv.org/abs/2504.09081v2), [pdf](http://arxiv.org/pdf/2504.09081v2.pdf), cication: [**-1**](None) 

	 *Prabhat Pandey, Rupak Vignesh Swaminathan, K V Vijay Girish, ..., Grant P. Strimel, Andreas Schwarz* · ([huggingface](https://huggingface.co/datasets/amazon-agi/SIFT-50M))
- **Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding 
  and Expert Reasoning Abilities**, `arXiv, 2503.03983`, [arxiv](http://arxiv.org/abs/2503.03983v1), [pdf](http://arxiv.org/pdf/2503.03983v1.pdf), cication: [**-1**](None) 

	 *Sreyan Ghosh, Zhifeng Kong, Sonal Kumar, ..., Dinesh Manocha, Bryan Catanzaro* · ([huggingface](https://huggingface.co/spaces/nvidia/audio-flamingo-2)) · ([audio-flamingo](https://github.com/NVIDIA/audio-flamingo) - NVIDIA) ![Star](https://img.shields.io/github/stars/NVIDIA/audio-flamingo.svg?style=social&label=Star)
- **Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study 
  on Audio Question Answering**, `arXiv, 2503.11197`, [arxiv](http://arxiv.org/abs/2503.11197v1), [pdf](http://arxiv.org/pdf/2503.11197v1.pdf), cication: [**-1**](None) 

	 *Gang Li, Jizhong Liu, Heinrich Dinkel, ..., Junbo Zhang, Jian Luan* · ([r1-aqa](https://github.com/xiaomi/r1-aqa) - xiaomi) ![Star](https://img.shields.io/github/stars/xiaomi/r1-aqa.svg?style=social&label=Star) · ([huggingface](https://huggingface.co/mispeech/r1-aqa.))
- 🌟 **Slamming: Training a Speech Language Model on One GPU in a Day**, `arXiv, 2502.15814`, [arxiv](http://arxiv.org/abs/2502.15814v1), [pdf](http://arxiv.org/pdf/2502.15814v1.pdf), cication: [**-1**](None) 

	 *Gallil Maimon, Avishai Elmakies, Yossi Adi* · ([pages.cs.huji.ac](https://pages.cs.huji.ac.il/adiyoss-lab/slamming/)) · ([arxiv](https://arxiv.org/abs/2502.15814)) · ([slamkit](https://github.com/slp-rl/slamkit) - slp-rl) ![Star](https://img.shields.io/github/stars/slp-rl/slamkit.svg?style=social&label=Star)
- **Audio-FLAN: A Preliminary Release**, `arXiv, 2502.16584`, [arxiv](http://arxiv.org/abs/2502.16584v1), [pdf](http://arxiv.org/pdf/2502.16584v1.pdf), cication: [**-1**](None) 

	 *Liumeng Xue, Ziya Zhou, Jiahao Pan, ..., Yike Guo, Wei Xue* · ([Audio-FLAN](https://github.com/lmxue/Audio-FLAN) - lmxue) ![Star](https://img.shields.io/github/stars/lmxue/Audio-FLAN.svg?style=social&label=Star)
- **video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model**, `arXiv, 2502.11775`, [arxiv](http://arxiv.org/abs/2502.11775v1), [pdf](http://arxiv.org/pdf/2502.11775v1.pdf), cication: [**-1**](None) 

	 *Guangzhi Sun, Yudong Yang, Jimin Zhuang, ..., Zejun MA, Chao Zhang* · ([video-SALMONN-o1](https://github.com/BriansIDP/video-SALMONN-o1) - BriansIDP) ![Star](https://img.shields.io/github/stars/BriansIDP/video-SALMONN-o1.svg?style=social&label=Star)
- 🌟 **Soundwave: Less is More for Speech-Text Alignment in LLMs**, `arXiv, 2502.12900`, [arxiv](http://arxiv.org/abs/2502.12900v1), [pdf](http://arxiv.org/pdf/2502.12900v1.pdf), cication: [**-1**](None) 

	 *Yuhao Zhang, Zhiheng Liu, Fan Bu, ..., Benyou Wang, Haizhou Li* · ([Soundwave.](https://github.com/FreedomIntelligence/Soundwave.) - FreedomIntelligence) ![Star](https://img.shields.io/github/stars/FreedomIntelligence/Soundwave..svg?style=social&label=Star)
- **Meta Audiobox Aesthetics: Unified Automatic Quality Assessment for 
  Speech, Music, and Sound**, `arXiv, 2502.05139`, [arxiv](http://arxiv.org/abs/2502.05139v1), [pdf](http://arxiv.org/pdf/2502.05139v1.pdf), cication: [**-1**](None) 

	 *Andros Tjandra, Yi-Chiao Wu, Baishan Guo, ..., Ann Lee, Wei-Ning Hsu* · ([audiobox-aesthetics](https://github.com/facebookresearch/audiobox-aesthetics) - facebookresearch) ![Star](https://img.shields.io/github/stars/facebookresearch/audiobox-aesthetics.svg?style=social&label=Star) · ([ai.meta](https://ai.meta.com/research/publications/meta-audiobox-aesthetics-unified-automatic-quality-assessment-for-speech-music-and-sound/))
- **High-Fidelity Simultaneous Speech-To-Speech Translation**, `arXiv, 2502.03382`, [arxiv](http://arxiv.org/abs/2502.03382v1), [pdf](http://arxiv.org/pdf/2502.03382v1.pdf), cication: [**-1**](None) 

	 *Tom Labiausse, Laurent Mazaré, Edouard Grave, ..., Alexandre Défossez, Neil Zeghidour* · ([hibiki](https://github.com/kyutai-labs/hibiki) - kyutai-labs) ![Star](https://img.shields.io/github/stars/kyutai-labs/hibiki.svg?style=social&label=Star) · ([reddit](https://www.reddit.com/r/LocalLLaMA/comments/1ij35u7/hibiki_by_kyutai_a_simultaneous_speechtospeech/))
- **OSUM: Advancing Open Speech Understanding Models with Limited Resources 
  in Academia**, `arXiv, 2501.13306`, [arxiv](http://arxiv.org/abs/2501.13306v1), [pdf](http://arxiv.org/pdf/2501.13306v1.pdf), cication: [**-1**](None) 

	 *Xuelong Geng, Kun Wei, Qijie Shao, ..., Li Zhang, Lei Xie*
- **Audio-CoT: Exploring Chain-of-Thought Reasoning in Large Audio Language 
  Model**, `arXiv, 2501.07246`, [arxiv](http://arxiv.org/abs/2501.07246v1), [pdf](http://arxiv.org/pdf/2501.07246v1.pdf), cication: [**-1**](None) 

	 *Ziyang Ma, Zhuo Chen, Yuping Wang, ..., Eng Siong Chng, Xie Chen*
- **State-Space Large Audio Language Models**, `arXiv, 2411.15685`, [arxiv](http://arxiv.org/abs/2411.15685v1), [pdf](http://arxiv.org/pdf/2411.15685v1.pdf), cication: [**-1**](None) 

	 *Saurabhchand Bhati, Yuan Gong, Leonid Karlinsky, ..., Rogerio Feris, James Glass*
- 🌟 **Scaling Speech-Text Pre-training with Synthetic Interleaved Data**, `arXiv, 2411.17607`, [arxiv](http://arxiv.org/abs/2411.17607v1), [pdf](http://arxiv.org/pdf/2411.17607v1.pdf), cication: [**-1**](None) 

	 *Aohan Zeng, Zhengxiao Du, Mingdao Liu, ..., Yuxiao Dong, Jie Tang*
- 🌟 **A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks 
  with Large Language Models**, `arXiv, 2411.08742`, [arxiv](http://arxiv.org/abs/2411.08742v1), [pdf](http://arxiv.org/pdf/2411.08742v1.pdf), cication: [**-1**](None) 

	 *Dingdong Wang, Mingyu Cui, Dongchao Yang, ..., Xueyuan Chen, Helen Meng*
- **Roadmap towards Superhuman Speech Understanding using Large Language 
  Models**, `arXiv, 2410.13268`, [arxiv](http://arxiv.org/abs/2410.13268v1), [pdf](http://arxiv.org/pdf/2410.13268v1.pdf), cication: [**-1**](None)

	 *Fan Bu, Yuhao Zhang, Xidong Wang, ..., Qun Liu, Haizhou Li*

## Survey

- **Audio-Language Models for Audio-Centric Tasks: A survey**, `arXiv, 2501.15177`, [arxiv](http://arxiv.org/abs/2501.15177v1), [pdf](http://arxiv.org/pdf/2501.15177v1.pdf), cication: [**-1**](None) 

	 *Yi Su, Jisheng Bai, Qisheng Xu, ..., Kele Xu, Yong Dou*
- **Towards Controllable Speech Synthesis in the Era of Large Language 
  Models: A Survey**, `arXiv, 2412.06602`, [arxiv](http://arxiv.org/abs/2412.06602v1), [pdf](http://arxiv.org/pdf/2412.06602v1.pdf), cication: [**-1**](None) 

	 *Tianxin Xie, Yan Rong, Pengfei Zhang, ..., Li Liu*
- **Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for 
  Measuring the Capabilities of Spoken Language Models with 180 Tasks**, `arXiv, 2411.05361`, [arxiv](http://arxiv.org/abs/2411.05361v1), [pdf](http://arxiv.org/pdf/2411.05361v1.pdf), cication: [**-1**](None) 

	 *Chien-yu Huang, Wei-Chih Chen, Shu-wen Yang, ..., Shinji Watanabe, Hung-yi Lee*

## Evaluation

- **What Do Speech Foundation Models Not Learn About Speech?**, `arXiv, 2410.12948`, [arxiv](http://arxiv.org/abs/2410.12948v1), [pdf](http://arxiv.org/pdf/2410.12948v1.pdf), cication: [**-1**](None) 

	 *Abdul Waheed, Hanin Atwany, Bhiksha Raj, ..., Rita Singh*
- **Audio Is the Achilles' Heel: Red Teaming Audio Large Multimodal Models**, `arXiv, 2410.23861`, [arxiv](http://arxiv.org/abs/2410.23861v1), [pdf](http://arxiv.org/pdf/2410.23861v1.pdf), cication: [**-1**](None) 

	 *Hao Yang, Lizhen Qu, Ehsan Shareghi, ..., Gholamreza Haffari*
- **MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark**, `arXiv, 2410.19168`, [arxiv](http://arxiv.org/abs/2410.19168v1), [pdf](http://arxiv.org/pdf/2410.19168v1.pdf), cication: [**-1**](None) 

	 *S Sakshi, Utkarsh Tyagi, Sonal Kumar, ..., Sreyan Ghosh, Dinesh Manocha* · ([sakshi113.github](https://sakshi113.github.io/mmau_homepage/))

## Projects

- [**OSUM**](https://github.com/ASLP-lab/OSUM) - ASLP-lab ![Star](https://img.shields.io/github/stars/ASLP-lab/OSUM.svg?style=social&label=Star) 

	 *Advancing Open Speech Understanding Models with Limited Resources in Academia*

## Toolkits


## Misc
## Misc
- [Qwen2-Audio is a SOTA small-scale multimodal model (AudioLM) that handles audio and text inputs, allowing you to have voice interactions without ASR modules](https://huggingface.co/NexaAIDev/Qwen2-Audio-7B-GGUF)  🤗 