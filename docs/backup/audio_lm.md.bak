# Audio Language Model

- [Audio Language Model](#audio-language-model) 
  - [Papers](#papers)
  - [Survey](#survey)
  - [Evaluation](#evaluation)
  - [Projects](#projects)
  - [Toolkits](#toolkits)
  - [Misc](#misc)


## Papers

- **video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model**, `arXiv, 2502.11775`, [arxiv](http://arxiv.org/abs/2502.11775v1), [pdf](http://arxiv.org/pdf/2502.11775v1.pdf), cication: [**-1**](None) 

	 *Guangzhi Sun, Yudong Yang, Jimin Zhuang, ..., Zejun MA, Chao Zhang* · ([video-SALMONN-o1](https://github.com/BriansIDP/video-SALMONN-o1) - BriansIDP) ![Star](https://img.shields.io/github/stars/BriansIDP/video-SALMONN-o1.svg?style=social&label=Star)
- 🌟 **Soundwave: Less is More for Speech-Text Alignment in LLMs**, `arXiv, 2502.12900`, [arxiv](http://arxiv.org/abs/2502.12900v1), [pdf](http://arxiv.org/pdf/2502.12900v1.pdf), cication: [**-1**](None) 

	 *Yuhao Zhang, Zhiheng Liu, Fan Bu, ..., Benyou Wang, Haizhou Li* · ([Soundwave.](https://github.com/FreedomIntelligence/Soundwave.) - FreedomIntelligence) ![Star](https://img.shields.io/github/stars/FreedomIntelligence/Soundwave..svg?style=social&label=Star)
- **Meta Audiobox Aesthetics: Unified Automatic Quality Assessment for 
  Speech, Music, and Sound**, `arXiv, 2502.05139`, [arxiv](http://arxiv.org/abs/2502.05139v1), [pdf](http://arxiv.org/pdf/2502.05139v1.pdf), cication: [**-1**](None) 

	 *Andros Tjandra, Yi-Chiao Wu, Baishan Guo, ..., Ann Lee, Wei-Ning Hsu* · ([audiobox-aesthetics](https://github.com/facebookresearch/audiobox-aesthetics) - facebookresearch) ![Star](https://img.shields.io/github/stars/facebookresearch/audiobox-aesthetics.svg?style=social&label=Star) · ([ai.meta](https://ai.meta.com/research/publications/meta-audiobox-aesthetics-unified-automatic-quality-assessment-for-speech-music-and-sound/))
- **High-Fidelity Simultaneous Speech-To-Speech Translation**, `arXiv, 2502.03382`, [arxiv](http://arxiv.org/abs/2502.03382v1), [pdf](http://arxiv.org/pdf/2502.03382v1.pdf), cication: [**-1**](None) 

	 *Tom Labiausse, Laurent Mazaré, Edouard Grave, ..., Alexandre Défossez, Neil Zeghidour* · ([hibiki](https://github.com/kyutai-labs/hibiki) - kyutai-labs) ![Star](https://img.shields.io/github/stars/kyutai-labs/hibiki.svg?style=social&label=Star) · ([reddit](https://www.reddit.com/r/LocalLLaMA/comments/1ij35u7/hibiki_by_kyutai_a_simultaneous_speechtospeech/))
- **OSUM: Advancing Open Speech Understanding Models with Limited Resources 
  in Academia**, `arXiv, 2501.13306`, [arxiv](http://arxiv.org/abs/2501.13306v1), [pdf](http://arxiv.org/pdf/2501.13306v1.pdf), cication: [**-1**](None) 

	 *Xuelong Geng, Kun Wei, Qijie Shao, ..., Li Zhang, Lei Xie*
- **Audio-CoT: Exploring Chain-of-Thought Reasoning in Large Audio Language 
  Model**, `arXiv, 2501.07246`, [arxiv](http://arxiv.org/abs/2501.07246v1), [pdf](http://arxiv.org/pdf/2501.07246v1.pdf), cication: [**-1**](None) 

	 *Ziyang Ma, Zhuo Chen, Yuping Wang, ..., Eng Siong Chng, Xie Chen*
- **State-Space Large Audio Language Models**, `arXiv, 2411.15685`, [arxiv](http://arxiv.org/abs/2411.15685v1), [pdf](http://arxiv.org/pdf/2411.15685v1.pdf), cication: [**-1**](None) 

	 *Saurabhchand Bhati, Yuan Gong, Leonid Karlinsky, ..., Rogerio Feris, James Glass*
- 🌟 **Scaling Speech-Text Pre-training with Synthetic Interleaved Data**, `arXiv, 2411.17607`, [arxiv](http://arxiv.org/abs/2411.17607v1), [pdf](http://arxiv.org/pdf/2411.17607v1.pdf), cication: [**-1**](None) 

	 *Aohan Zeng, Zhengxiao Du, Mingdao Liu, ..., Yuxiao Dong, Jie Tang*
- 🌟 **A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks 
  with Large Language Models**, `arXiv, 2411.08742`, [arxiv](http://arxiv.org/abs/2411.08742v1), [pdf](http://arxiv.org/pdf/2411.08742v1.pdf), cication: [**-1**](None) 

	 *Dingdong Wang, Mingyu Cui, Dongchao Yang, ..., Xueyuan Chen, Helen Meng*
- **Roadmap towards Superhuman Speech Understanding using Large Language 
  Models**, `arXiv, 2410.13268`, [arxiv](http://arxiv.org/abs/2410.13268v1), [pdf](http://arxiv.org/pdf/2410.13268v1.pdf), cication: [**-1**](None)

	 *Fan Bu, Yuhao Zhang, Xidong Wang, ..., Qun Liu, Haizhou Li*

## Survey

- **Audio-Language Models for Audio-Centric Tasks: A survey**, `arXiv, 2501.15177`, [arxiv](http://arxiv.org/abs/2501.15177v1), [pdf](http://arxiv.org/pdf/2501.15177v1.pdf), cication: [**-1**](None) 

	 *Yi Su, Jisheng Bai, Qisheng Xu, ..., Kele Xu, Yong Dou*
- **Towards Controllable Speech Synthesis in the Era of Large Language 
  Models: A Survey**, `arXiv, 2412.06602`, [arxiv](http://arxiv.org/abs/2412.06602v1), [pdf](http://arxiv.org/pdf/2412.06602v1.pdf), cication: [**-1**](None) 

	 *Tianxin Xie, Yan Rong, Pengfei Zhang, ..., Li Liu*
- **Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for 
  Measuring the Capabilities of Spoken Language Models with 180 Tasks**, `arXiv, 2411.05361`, [arxiv](http://arxiv.org/abs/2411.05361v1), [pdf](http://arxiv.org/pdf/2411.05361v1.pdf), cication: [**-1**](None) 

	 *Chien-yu Huang, Wei-Chih Chen, Shu-wen Yang, ..., Shinji Watanabe, Hung-yi Lee*

## Evaluation

- **What Do Speech Foundation Models Not Learn About Speech?**, `arXiv, 2410.12948`, [arxiv](http://arxiv.org/abs/2410.12948v1), [pdf](http://arxiv.org/pdf/2410.12948v1.pdf), cication: [**-1**](None) 

	 *Abdul Waheed, Hanin Atwany, Bhiksha Raj, ..., Rita Singh*
- **Audio Is the Achilles' Heel: Red Teaming Audio Large Multimodal Models**, `arXiv, 2410.23861`, [arxiv](http://arxiv.org/abs/2410.23861v1), [pdf](http://arxiv.org/pdf/2410.23861v1.pdf), cication: [**-1**](None) 

	 *Hao Yang, Lizhen Qu, Ehsan Shareghi, ..., Gholamreza Haffari*
- **MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark**, `arXiv, 2410.19168`, [arxiv](http://arxiv.org/abs/2410.19168v1), [pdf](http://arxiv.org/pdf/2410.19168v1.pdf), cication: [**-1**](None) 

	 *S Sakshi, Utkarsh Tyagi, Sonal Kumar, ..., Sreyan Ghosh, Dinesh Manocha* · ([sakshi113.github](https://sakshi113.github.io/mmau_homepage/))

## Projects

- [**OSUM**](https://github.com/ASLP-lab/OSUM) - ASLP-lab ![Star](https://img.shields.io/github/stars/ASLP-lab/OSUM.svg?style=social&label=Star) 

	 *Advancing Open Speech Understanding Models with Limited Resources in Academia*

## Toolkits


## Misc
## Misc
- [Qwen2-Audio is a SOTA small-scale multimodal model (AudioLM) that handles audio and text inputs, allowing you to have voice interactions without ASR modules](https://huggingface.co/NexaAIDev/Qwen2-Audio-7B-GGUF)  🤗 