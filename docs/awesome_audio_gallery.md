# Awesome Audio Gallery

- [Awesome Audio Gallery](#awesome-audio-gallery)
  - [Courses and Tutorials](#courses-and-tutorials)
    - [DLHLP 2020 Spring](#dlhlp-2020-spring)
    - [Large Audio Model](#large-audio-model)
  - [Speech Translation](#speech-translation)
  - [Toolkits](#toolkits)
  - [Dataset](#dataset)
    - [TTS](#tts)
  - [Audio Techs](#audio-techs)

## Detection
- **Detecting Multimedia Generated by Large AI Models: A Survey**, `arXiv, 2402.00045`, [arxiv](http://arxiv.org/abs/2402.00045v1), [pdf](http://arxiv.org/pdf/2402.00045v1.pdf), cication: [**-1**](None)

	 *Li Lin, Neeraj Gupta, Yue Zhang, Hainan Ren, Chun-Hao Liu, Feng Ding, Xin Wang, Xin Li, Luisa Verdoliva, Shu Hu* ¬∑ ([Detect-LAIM-generated-Multimedia-Survey](https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey) - Purdue-M2) ![Star](https://img.shields.io/github/stars/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey.svg?style=social&label=Star)
- **Proactive Detection of Voice Cloning with Localized Watermarking**, `arXiv, 2401.17264`, [arxiv](http://arxiv.org/abs/2401.17264v1), [pdf](http://arxiv.org/pdf/2401.17264v1.pdf), cication: [**-1**](None)

	 *Robin San Roman, Pierre Fernandez, Alexandre D√©fossez, Teddy Furon, Tuan Tran, Hady Elsahar* ¬∑ ([audioseal](https://github.com/facebookresearch/audioseal) - facebookresearch) ![Star](https://img.shields.io/github/stars/facebookresearch/audioseal.svg?style=social&label=Star)

## Courses and Tutorials

### DLHLP 2020 Spring
- [DLHLP 2020 Spring](https://speech.ee.ntu.edu.tw/~hylee/dlhlp/2020-spring.php)
- [[DLHLP 2020] ÊùéÂÆèÊØÖËÄÅÂ∏à2020Êò•ËØæÁ®ã-ËØ≠Èü≥ËØÜÂà´-ËØ≠Èü≥ÂêàÊàê-ËØ≠Èü≥ÂàÜÁ¶ª\_ÂìîÂì©ÂìîÂì©\_bilibili](https://www.bilibili.com/video/BV1hZ4y1w7j1/?spm_id_from=333.788.top_right_bar_window_custom_collection.content.click&vd_source=1453a06a1e0b377f5c40946333b4423a)
- [[DLHLP 2020] Vocoder (Áî±Âä©ÊïôË®±ÂçöÁ´£ÂêåÂ≠∏Ë¨õÊéà)\_ÂìîÂì©ÂìîÂì©\_bilibili](https://www.bilibili.com/video/BV1hZ4y1w7j1?p=11&vd_source=1453a06a1e0b377f5c40946333b4423a)
- TTS Intro: [[DLHLP 2020] Speech Synthesis (1/2) - Tacotron - YouTube](https://www.youtube.com/watch?v=DMxKeHW8KdM&ab_channel=Hung-yiLee)

### Large Audio Model
- [„ÄêÊ©üÂô®Â≠∏Áøí2023„ÄëË™ûÈü≥Âü∫Áü≥Ê®°Âûã (Âä©ÊïôÂºµÂá±ÁÇ∫Ë¨õÊéà) (1/2) - YouTube](https://www.youtube.com/watch?v=m7Be7ppR6q0&ab_channel=Hung-yiLee)
- [„ÄêÊ©üÂô®Â≠∏Áøí2023„ÄëË™ûÈü≥Âü∫Áü≥Ê®°Âûã (Âä©ÊïôÂºµÂá±ÁÇ∫Ë¨õÊéà) (2/2) - YouTube](https://www.youtube.com/watch?v=HTAq-CPrU5s&ab_channel=Hung-yiLee)
- [https://speech.ee.ntu.edu.tw/\~hylee/ml/ml2023-course-data/ÂºµÂá±Áà≤-x-Ê©üÂô®Â≠∏Áøí-x-Ë™ûÈü≥Âü∫Áü≥Ê®°Âûã.pdf](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/%E5%BC%B5%E5%87%B1%E7%88%B2-x-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-x-%E8%AA%9E%E9%9F%B3%E5%9F%BA%E7%9F%B3%E6%A8%A1%E5%9E%8B.pdf)

### Other
- [**awesome-speech-recognition-speech-synthesis-papers**](https://github.com/zzw922cn/awesome-speech-recognition-speech-synthesis-papers) - zzw922cn ![Star](https://img.shields.io/github/stars/zzw922cn/awesome-speech-recognition-speech-synthesis-papers.svg?style=social&label=Star)
- [**Awesome-Singing-Voice-Synthesis-and-Singing-Voice-Conversion**](https://github.com/guan-yuan/Awesome-Singing-Voice-Synthesis-and-Singing-Voice-Conversion) - guan-yuan ![Star](https://img.shields.io/github/stars/guan-yuan/Awesome-Singing-Voice-Synthesis-and-Singing-Voice-Conversion.svg?style=social&label=Star)
- [**survey**](https://github.com/tts-tutorial/survey) - tts-tutorial ![Star](https://img.shields.io/github/stars/tts-tutorial/survey.svg?style=social&label=Star)

	 *A Survey on Neural Speech Synthesis*
- [**interspeech2022**](https://github.com/tts-tutorial/interspeech2022) - tts-tutorial ![Star](https://img.shields.io/github/stars/tts-tutorial/interspeech2022.svg?style=social&label=Star)
- [INTERSPEECH\_Tutorial\_TTS.pdf](https://tts-tutorial.github.io/interspeech2022/INTERSPEECH_Tutorial_TTS.pdf)
- [INTERSPEECH\_Tutorial\_VC.pdf](https://tts-tutorial.github.io/interspeech2022/INTERSPEECH_Tutorial_VC.pdf)
- [https://www.microsoft.com/en-us/research/uploads/prod/2022/12/Generative-Models-for-TTS.pdf](https://www.microsoft.com/en-us/research/uploads/prod/2022/12/Generative-Models-for-TTS.pdf)
- [**AudioGPT**](https://github.com/AIGC-Audio/AudioGPT) - AIGC-Audio ![Star](https://img.shields.io/github/stars/AIGC-Audio/AudioGPT.svg?style=social&label=Star)

## Speech Translation
- [**gentranslate**](https://github.com/yuchen005/gentranslate) - yuchen005 ![Star](https://img.shields.io/github/stars/yuchen005/gentranslate.svg?style=social&label=Star)

	 *Code for paper "GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators"*
- **PolyVoice: Language Models for Speech to Speech Translation**, `arXiv, 2306.02982`, [arxiv](http://arxiv.org/abs/2306.02982v2), [pdf](http://arxiv.org/pdf/2306.02982v2.pdf), cication: [**-1**](None)

	 *Qianqian Dong, Zhiying Huang, Qiao Tian, Chen Xu, Tom Ko, Yunlong Zhao, Siyuan Feng, Tang Li, Kexin Wang, Xuxin Cheng* ¬∑ ([speechtranslation.github](https://speechtranslation.github.io/polyvoice/))
- [**fairseq**](https://github.com/facebookresearch/fairseq/tree/ust/examples/hokkien) - facebookresearch ![Star](https://img.shields.io/github/stars/facebookresearch/fairseq.svg?style=social&label=Star)

## Toolkits
- [**NeMo-text-processing**](https://github.com/NVIDIA/NeMo-text-processing?tab=readme-ov-file) - NVIDIA ![Star](https://img.shields.io/github/stars/NVIDIA/NeMo-text-processing.svg?style=social&label=Star)

	 *NeMo text processing for ASR and TTS*
- [**OpenPhonemizer**](https://github.com/NeuralVox/OpenPhonemizer) - NeuralVox ![Star](https://img.shields.io/github/stars/NeuralVox/OpenPhonemizer.svg?style=social&label=Star)

	 *Permissively licensed, open sourced, local IPA Phonemizer (G2P) powered by deep learning.*
- [**fullstop-deep-punctuation-prediction**](https://github.com/oliverguhr/fullstop-deep-punctuation-prediction) - oliverguhr ![Star](https://img.shields.io/github/stars/oliverguhr/fullstop-deep-punctuation-prediction.svg?style=social&label=Star)

	 *A model that predicts the punctuation of English, Italian, French and German texts.* ¬∑ ([huggingface](https://huggingface.co/oliverguhr/fullstop-punctuation-multilang-large))
- [**nendo**](https://github.com/okio-ai/nendo) - okio-ai ![Star](https://img.shields.io/github/stars/okio-ai/nendo.svg?style=social&label=Star)

	 *The Nendo AI Audio Tool Suite*
- [**deepfilternet**](https://github.com/rikorose/deepfilternet) - rikorose ![Star](https://img.shields.io/github/stars/rikorose/deepfilternet.svg?style=social&label=Star)

	 *Noise supression using deep filtering*
- [**CharsiuG2P**](https://github.com/lingjzhu/CharsiuG2P) - lingjzhu ![Star](https://img.shields.io/github/stars/lingjzhu/CharsiuG2P.svg?style=social&label=Star)

	 *Multilingual G2P in 100 languages*
- [(Inverse) Text Normalization ‚Äî NVIDIA NeMo](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/text_normalization/intro.html)
- [**audio-preprocess**](https://github.com/fishaudio/audio-preprocess) - fishaudio ![Star](https://img.shields.io/github/stars/fishaudio/audio-preprocess.svg?style=social&label=Star)

	 *Preprocess Audio for training*
- [**pyloudnorm**](https://github.com/csteinmetz1/pyloudnorm) - csteinmetz1 ![Star](https://img.shields.io/github/stars/csteinmetz1/pyloudnorm.svg?style=social&label=Star)

	 *Flexible audio loudness meter in Python with implementation of ITU-R BS.1770-4 loudness algorithm*
- [**ultimatevocalremovergui**](https://github.com/Anjok07/ultimatevocalremovergui) - Anjok07 ![Star](https://img.shields.io/github/stars/Anjok07/ultimatevocalremovergui.svg?style=social&label=Star)

	 *GUI for a Vocal Remover that uses Deep Neural Networks.*
- **Amphion: An Open-Source Audio, Music and Speech Generation Toolkit**, `arXiv, 2312.09911`, [arxiv](http://arxiv.org/abs/2312.09911v1), [pdf](http://arxiv.org/pdf/2312.09911v1.pdf), cication: [**-1**](None)

	 *Xueyao Zhang, Liumeng Xue, Yuancheng Wang, Yicheng Gu, Xi Chen, Zihao Fang, Haopeng Chen, Lexiao Zou, Chaoren Wang, Jun Han* ¬∑ ([huggingface](https://huggingface.co/amphion)) ¬∑ ([Amphion](https://github.com/open-mmlab/Amphion/tree/main) - open-mmlab) ![Star](https://img.shields.io/github/stars/open-mmlab/Amphion.svg?style=social&label=Star)
- [**resemble-enhance**](https://github.com/resemble-ai/resemble-enhance) - resemble-ai ![Star](https://img.shields.io/github/stars/resemble-ai/resemble-enhance.svg?style=social&label=Star)

	 *AI powered speech denoising and enhancement*
- [**awesome-python**](https://github.com/vinta/awesome-python#audio) - vinta ![Star](https://img.shields.io/github/stars/vinta/awesome-python.svg?style=social&label=Star)

	 *A curated list of awesome Python frameworks, libraries, software and resources*
- [**pyannote-audio**](https://github.com/pyannote/pyannote-audio) - pyannote ![Star](https://img.shields.io/github/stars/pyannote/pyannote-audio.svg?style=social&label=Star)

	 *Neural building blocks for speaker diarization: speech activity detection, speaker change detection, overlapped speech detection, speaker embedding*
- [**audiocraft**](https://github.com/facebookresearch/audiocraft) - facebookresearch ![Star](https://img.shields.io/github/stars/facebookresearch/audiocraft.svg?style=social&label=Star)

	 *Audiocraft is a library for audio processing and generation with deep learning. It features the state-of-the-art EnCodec audio compressor / tokenizer, along with MusicGen, a simple and controllable music generation LM with textual and melodic conditioning.*
- [**audio-slicer**](https://github.com/openvpi/audio-slicer) - openvpi ![Star](https://img.shields.io/github/stars/openvpi/audio-slicer.svg?style=social&label=Star)

	 *Python script that slices audio with silence detection*
- [**autocut**](https://github.com/mli/autocut/blob/main/autocut/transcribe.py) - mli ![Star](https://img.shields.io/github/stars/mli/autocut.svg?style=social&label=Star)
- [**phonemizer**](https://github.com/bootphon/phonemizer) - bootphon ![Star](https://img.shields.io/github/stars/bootphon/phonemizer.svg?style=social&label=Star)

	 *Simple text to phones converter for multiple languages*
- [**g2pM**](https://github.com/kakaobrain/g2pM) - kakaobrain ![Star](https://img.shields.io/github/stars/kakaobrain/g2pM.svg?style=social&label=Star)

	 *A Neural Grapheme-to-Phoneme Conversion Package for Mandarin Chinese Based on a New Open Benchmark Dataset*
- [**g2pC**](https://github.com/Kyubyong/g2pC) - Kyubyong ![Star](https://img.shields.io/github/stars/Kyubyong/g2pC.svg?style=social&label=Star)

	 *g2pC: A Context-aware Grapheme-to-Phoneme Conversion module for Chinese*
- [**AcademiCodec**](https://github.com/yangdongchao/AcademiCodec) - yangdongchao ![Star](https://img.shields.io/github/stars/yangdongchao/AcademiCodec.svg?style=social&label=Star)

	 *AcademiCodec: An Open Source Audio Codec Model for Academic Research*
- [**Python-Wrapper-for-World-Vocoder**](https://github.com/JeremyCCHsu/Python-Wrapper-for-World-Vocoder) - JeremyCCHsu ![Star](https://img.shields.io/github/stars/JeremyCCHsu/Python-Wrapper-for-World-Vocoder.svg?style=social&label=Star)

	 *A Python wrapper for the high-quality vocoder "World"*
- [**g2p-kd**](https://github.com/sigmeta/g2p-kd) - sigmeta ![Star](https://img.shields.io/github/stars/sigmeta/g2p-kd.svg?style=social&label=Star)

	 *Token-Level Ensemble Distillation for Grapheme-to-Phoneme Conversion*
- [**speechbrain**](https://github.com/speechbrain/speechbrain) - speechbrain ![Star](https://img.shields.io/github/stars/speechbrain/speechbrain.svg?style=social&label=Star)

	 *A PyTorch-based Speech Toolkit*
- [**ffmpeg-normalize**](https://github.com/slhck/ffmpeg-normalize) - slhck ![Star](https://img.shields.io/github/stars/slhck/ffmpeg-normalize.svg?style=social&label=Star)

	 *Audio Normalization for Python/ffmpeg*
- [**Montreal-Forced-Aligner**](https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner) - MontrealCorpusTools ![Star](https://img.shields.io/github/stars/MontrealCorpusTools/Montreal-Forced-Aligner.svg?style=social&label=Star)

	 *Command line utility for forced alignment using Kaldi*
- [**WeTextProcessing**](https://github.com/wenet-e2e/WeTextProcessing) - wenet-e2e ![Star](https://img.shields.io/github/stars/wenet-e2e/WeTextProcessing.svg?style=social&label=Star)

	 *Text Normalization & Inverse Text Normalization*
- [**python-pinyin**](https://github.com/mozillazg/python-pinyin) - mozillazg ![Star](https://img.shields.io/github/stars/mozillazg/python-pinyin.svg?style=social&label=Star)

	 *Ê±âÂ≠óËΩ¨ÊãºÈü≥(pypinyin)*
- [**pypinyin-g2pW**](https://github.com/mozillazg/pypinyin-g2pW) - mozillazg ![Star](https://img.shields.io/github/stars/mozillazg/pypinyin-g2pW.svg?style=social&label=Star)

	 *Âü∫‰∫é g2pW ÊèêÂçá pypinyin ÁöÑÂáÜÁ°ÆÊÄß*

## Dataset
- [**yodas**](https://huggingface.co/datasets/espnet/yodas) - espnet ü§ó
- **An Automated End-to-End Open-Source Software for High-Quality
  Text-to-Speech Dataset Generation**, `arXiv, 2402.16380`, [arxiv](http://arxiv.org/abs/2402.16380v1), [pdf](http://arxiv.org/pdf/2402.16380v1.pdf), cication: [**-1**](None)

	 *Ahmet Gunduz, Kamer Ali Yuksel, Kareem Darwish, Golara Javadi, Fabio Minazzi, Nicola Sobieski, Sebastien Bratieres*
- [**common_voice_16_0**](https://huggingface.co/datasets/mozilla-foundation/common_voice_16_0) - mozilla-foundation ü§ó
- [**GigaSpeech**](https://github.com/SpeechColab/GigaSpeech) - SpeechColab ![Star](https://img.shields.io/github/stars/SpeechColab/GigaSpeech.svg?style=social&label=Star)

	 *Large, modern dataset for speech recognition*
- [‰∏≠Ëã±ÊñáÊï∞ÊçÆÊî∂ÈõÜ](https://yqli.tech/page/data.html)
- [**voice_datasets**](https://github.com/jim-schwoebel/voice_datasets) - jim-schwoebel ![Star](https://img.shields.io/github/stars/jim-schwoebel/voice_datasets.svg?style=social&label=Star)

	 *üîä A comprehensive list of open-source datasets for voice and sound computing (95+ datasets).*

### TTS
- **LibriTTS-R: A Restored Multi-Speaker Text-to-Speech Corpus**, `arXiv, 2305.18802`, [arxiv](http://arxiv.org/abs/2305.18802v1), [pdf](http://arxiv.org/pdf/2305.18802v1.pdf), cication: [**-1**](None)

	 *Yuma Koizumi, Heiga Zen, Shigeki Karita, Yifan Ding, Kohei Yatabe, Nobuyuki Morioka, Michiel Bacchiani, Yu Zhang, Wei Han, Ankur Bapna* ¬∑ ([openslr](http://www.openslr.org/141/)) ¬∑ ([google.github](https://google.github.io/df-conformer/librittsr/))
- **Libri-Light: A Benchmark for ASR with Limited or No Supervision**, `icassp 2020-2020 ieee international conference on acoustics¬†‚Ä¶, 2020`, [arxiv](http://arxiv.org/abs/1912.07875v1), [pdf](http://arxiv.org/pdf/1912.07875v1.pdf), cication: [**483**](https://scholar.google.com/scholar?cites=596221056952134740&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Jacob Kahn, Morgane Rivi√®re, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, Pierre-Emmanuel Mazar√©, Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen* ¬∑ ([libri-light](https://github.com/facebookresearch/libri-light/tree/main) - facebookresearch) ![Star](https://img.shields.io/github/stars/facebookresearch/libri-light.svg?style=social&label=Star)
- **MLS: A Large-Scale Multilingual Dataset for Speech Research**, `arXiv, 2012.03411`, [arxiv](http://arxiv.org/abs/2012.03411v2), [pdf](http://arxiv.org/pdf/2012.03411v2.pdf), cication: [**-1**](None)

	 *Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, Ronan Collobert* ¬∑ ([openslr](http://www.openslr.org/94/))
	- Multilingual LibriSpeech (MLS) dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of 8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese, Polish. 
	- segment the audio files into 10-20 second segments 
- **LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech**, `arXiv, 1904.02882`, [arxiv](http://arxiv.org/abs/1904.02882v1), [pdf](http://arxiv.org/pdf/1904.02882v1.pdf), cication: [**-1**](None)

	 *Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J. Weiss, Ye Jia, Zhifeng Chen, Yonghui Wu*
- **Hi-Fi Multi-Speaker English TTS Dataset**, `arXiv, 2104.01497`, [arxiv](http://arxiv.org/abs/2104.01497v3), [pdf](http://arxiv.org/pdf/2104.01497v3.pdf), cication: [**-1**](None)

	 *Evelina Bakhturina, Vitaly Lavrukhin, Boris Ginsburg, Yang Zhang* ¬∑ ([openslr](https://www.openslr.org/109/))

## Audio Techs
- **EfficientSpeech: An On-Device Text to Speech Model**, `arXiv, 2305.13905`, [arxiv](http://arxiv.org/abs/2305.13905v1), [pdf](http://arxiv.org/pdf/2305.13905v1.pdf), cication: [**-1**](None)

	 *Rowel Atienza*
- **Visual-Aware Text-to-Speech**, `arXiv, 2306.12020`, [arxiv](http://arxiv.org/abs/2306.12020v1), [pdf](http://arxiv.org/pdf/2306.12020v1.pdf), cication: [**-1**](None)

	 *Mohan Zhou, Yalong Bai, Wei Zhang, Ting Yao, Tiejun Zhao, Tao Mei*
- **Proactive Detection of Voice Cloning with Localized Watermarking**, `arXiv, 2401.17264`, [arxiv](http://arxiv.org/abs/2401.17264v1), [pdf](http://arxiv.org/pdf/2401.17264v1.pdf), cication: [**-1**](None)

	 *Robin San Roman, Pierre Fernandez, Alexandre D√©fossez, Teddy Furon, Tuan Tran, Hady Elsahar*
- **FADI-AEC: Fast Score Based Diffusion Model Guided by Far-end Signal for
  Acoustic Echo Cancellation**, `arXiv, 2401.04283`, [arxiv](http://arxiv.org/abs/2401.04283v1), [pdf](http://arxiv.org/pdf/2401.04283v1.pdf), cication: [**-1**](None)

	 *Yang Liu, Li Wan, Yun Li, Yiteng Huang, Ming Sun, James Luan, Yangyang Shi, Xin Lei*
- [**AudioSep**](https://github.com/Audio-AGI/AudioSep) - Audio-AGI ![Star](https://img.shields.io/github/stars/Audio-AGI/AudioSep.svg?style=social&label=Star)

	 *Official implementation of "Separate Anything You Describe"*
- **AudioSR: Versatile Audio Super-resolution at Scale**, `arXiv, 2309.07314`, [arxiv](http://arxiv.org/abs/2309.07314v1), [pdf](http://arxiv.org/pdf/2309.07314v1.pdf), cication: [**-1**](None)

	 *Haohe Liu, Ke Chen, Qiao Tian, Wenwu Wang, Mark D. Plumbley*

## Audio Visual
- [Fetching Title#8iex](https://arxiv.org/abs/2403.16276)
- **M$^3$AV: A Multimodal, Multigenre, and Multipurpose Audio-Visual
  Academic Lecture Dataset**, `arXiv, 2403.14168`, [arxiv](http://arxiv.org/abs/2403.14168v1), [pdf](http://arxiv.org/pdf/2403.14168v1.pdf), cication: [**-1**](None)

	 *Zhe Chen, Heyang Liu, Wenyi Yu, Guangzhi Sun, Hongcheng Liu, Ji Wu, Chao Zhang, Yu Wang, Yanfeng Wang*
- **Text-to-Audio Generation Synchronized with Videos**, `arXiv, 2403.07938`, [arxiv](http://arxiv.org/abs/2403.07938v1), [pdf](http://arxiv.org/pdf/2403.07938v1.pdf), cication: [**-1**](None)

	 *Shentong Mo, Jing Shi, Yapeng Tian*
- **Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion
  Latent Aligners**, `arXiv, 2402.17723`, [arxiv](http://arxiv.org/abs/2402.17723v1), [pdf](http://arxiv.org/pdf/2402.17723v1.pdf), cication: [**-1**](None)

	 *Yazhou Xing, Yingqing He, Zeyue Tian, Xintao Wang, Qifeng Chen* ¬∑ ([yzxing87.github](https://yzxing87.github.io/Seeing-and-Hearing/))
- [**vsp-llm**](https://github.com/sally-sh/vsp-llm) - sally-sh ![Star](https://img.shields.io/github/stars/sally-sh/vsp-llm.svg?style=social&label=Star)
- **RTFS-Net: Recurrent time-frequency modelling for efficient audio-visual
  speech separation**, `arXiv, 2309.17189`, [arxiv](http://arxiv.org/abs/2309.17189v3), [pdf](http://arxiv.org/pdf/2309.17189v3.pdf), cication: [**-1**](None)

	 *Samuel Pegg, Kai Li, Xiaolin Hu* ¬∑ ([jiqizhixin](https://www.jiqizhixin.com/articles/2024-03-06)) ¬∑ ([cslikai](https://cslikai.cn/RTFS-Net/AV-Model-Demo.html)) ¬∑ ([RTFS-Net](https://github.com/spkgyk/RTFS-Net) - spkgyk) ![Star](https://img.shields.io/github/stars/spkgyk/RTFS-Net.svg?style=social&label=Star)