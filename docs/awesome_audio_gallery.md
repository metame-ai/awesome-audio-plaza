# Awesome Audio Gallery

- [Awesome Audio Gallery](#awesome-audio-gallery)
  - [Courses and Tutorials](#courses-and-tutorials)
    - [DLHLP 2020 Spring](#dlhlp-2020-spring)
    - [Large Audio Model](#large-audio-model)
  - [Speech Translation](#speech-translation)
  - [Toolkits](#toolkits)
  - [Dataset](#dataset)
    - [TTS](#tts)
  - [Audio Techs](#audio-techs)

## Detection
- **Detecting Multimedia Generated by Large AI Models: A Survey**, `arXiv, 2402.00045`, [arxiv](http://arxiv.org/abs/2402.00045v1), [pdf](http://arxiv.org/pdf/2402.00045v1.pdf), cication: [**-1**](None)

	 *Li Lin, Neeraj Gupta, Yue Zhang, Hainan Ren, Chun-Hao Liu, Feng Ding, Xin Wang, Xin Li, Luisa Verdoliva, Shu Hu* ¬∑ ([Detect-LAIM-generated-Multimedia-Survey](https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey) - Purdue-M2) ![Star](https://img.shields.io/github/stars/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey.svg?style=social&label=Star)
- **Proactive Detection of Voice Cloning with Localized Watermarking**, `arXiv, 2401.17264`, [arxiv](http://arxiv.org/abs/2401.17264v1), [pdf](http://arxiv.org/pdf/2401.17264v1.pdf), cication: [**-1**](None)

	 *Robin San Roman, Pierre Fernandez, Alexandre D√©fossez, Teddy Furon, Tuan Tran, Hady Elsahar* ¬∑ ([audioseal](https://github.com/facebookresearch/audioseal) - facebookresearch) ![Star](https://img.shields.io/github/stars/facebookresearch/audioseal.svg?style=social&label=Star)

## Courses and Tutorials

### DLHLP 2020 Spring
- [DLHLP 2020 Spring](https://speech.ee.ntu.edu.tw/~hylee/dlhlp/2020-spring.php)
- [[DLHLP 2020] ÊùéÂÆèÊØÖËÄÅÂ∏à2020Êò•ËØæÁ®ã-ËØ≠Èü≥ËØÜÂà´-ËØ≠Èü≥ÂêàÊàê-ËØ≠Èü≥ÂàÜÁ¶ª\_ÂìîÂì©ÂìîÂì©\_bilibili](https://www.bilibili.com/video/BV1hZ4y1w7j1/?spm_id_from=333.788.top_right_bar_window_custom_collection.content.click&vd_source=1453a06a1e0b377f5c40946333b4423a)
- [[DLHLP 2020] Vocoder (Áî±Âä©ÊïôË®±ÂçöÁ´£ÂêåÂ≠∏Ë¨õÊéà)\_ÂìîÂì©ÂìîÂì©\_bilibili](https://www.bilibili.com/video/BV1hZ4y1w7j1?p=11&vd_source=1453a06a1e0b377f5c40946333b4423a)
- TTS Intro: [[DLHLP 2020] Speech Synthesis (1/2) - Tacotron - YouTube](https://www.youtube.com/watch?v=DMxKeHW8KdM&ab_channel=Hung-yiLee)

### Large Audio Model
- [„ÄêÊ©üÂô®Â≠∏Áøí2023„ÄëË™ûÈü≥Âü∫Áü≥Ê®°Âûã (Âä©ÊïôÂºµÂá±ÁÇ∫Ë¨õÊéà) (1/2) - YouTube](https://www.youtube.com/watch?v=m7Be7ppR6q0&ab_channel=Hung-yiLee)
- [„ÄêÊ©üÂô®Â≠∏Áøí2023„ÄëË™ûÈü≥Âü∫Áü≥Ê®°Âûã (Âä©ÊïôÂºµÂá±ÁÇ∫Ë¨õÊéà) (2/2) - YouTube](https://www.youtube.com/watch?v=HTAq-CPrU5s&ab_channel=Hung-yiLee)
- [https://speech.ee.ntu.edu.tw/\~hylee/ml/ml2023-course-data/ÂºµÂá±Áà≤-x-Ê©üÂô®Â≠∏Áøí-x-Ë™ûÈü≥Âü∫Áü≥Ê®°Âûã.pdf](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/%E5%BC%B5%E5%87%B1%E7%88%B2-x-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-x-%E8%AA%9E%E9%9F%B3%E5%9F%BA%E7%9F%B3%E6%A8%A1%E5%9E%8B.pdf)

## Speech Translation
- [**gentranslate**](https://github.com/yuchen005/gentranslate) - yuchen005 ![Star](https://img.shields.io/github/stars/yuchen005/gentranslate.svg?style=social&label=Star)

	 *Code for paper "GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators"*
- **PolyVoice: Language Models for Speech to Speech Translation**, `arXiv, 2306.02982`, [arxiv](http://arxiv.org/abs/2306.02982v2), [pdf](http://arxiv.org/pdf/2306.02982v2.pdf), cication: [**-1**](None)

	 *Qianqian Dong, Zhiying Huang, Qiao Tian, Chen Xu, Tom Ko, Yunlong Zhao, Siyuan Feng, Tang Li, Kexin Wang, Xuxin Cheng* ¬∑ ([speechtranslation.github](https://speechtranslation.github.io/polyvoice/))
- [**fairseq**](https://github.com/facebookresearch/fairseq/tree/ust/examples/hokkien) - facebookresearch ![Star](https://img.shields.io/github/stars/facebookresearch/fairseq.svg?style=social&label=Star)

## Toolkits
- [**nendo**](https://github.com/okio-ai/nendo) - okio-ai ![Star](https://img.shields.io/github/stars/okio-ai/nendo.svg?style=social&label=Star)

	 *The Nendo AI Audio Tool Suite*
- [**deepfilternet**](https://github.com/rikorose/deepfilternet) - rikorose ![Star](https://img.shields.io/github/stars/rikorose/deepfilternet.svg?style=social&label=Star)

	 *Noise supression using deep filtering*
- [**CharsiuG2P**](https://github.com/lingjzhu/CharsiuG2P) - lingjzhu ![Star](https://img.shields.io/github/stars/lingjzhu/CharsiuG2P.svg?style=social&label=Star)

	 *Multilingual G2P in 100 languages*
- [(Inverse) Text Normalization ‚Äî NVIDIA NeMo](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/text_normalization/intro.html)
- [**audio-preprocess**](https://github.com/fishaudio/audio-preprocess) - fishaudio ![Star](https://img.shields.io/github/stars/fishaudio/audio-preprocess.svg?style=social&label=Star)

	 *Preprocess Audio for training*
- [**pyloudnorm**](https://github.com/csteinmetz1/pyloudnorm) - csteinmetz1 ![Star](https://img.shields.io/github/stars/csteinmetz1/pyloudnorm.svg?style=social&label=Star)

	 *Flexible audio loudness meter in Python with implementation of ITU-R BS.1770-4 loudness algorithm*
- [**ultimatevocalremovergui**](https://github.com/Anjok07/ultimatevocalremovergui) - Anjok07 ![Star](https://img.shields.io/github/stars/Anjok07/ultimatevocalremovergui.svg?style=social&label=Star)

	 *GUI for a Vocal Remover that uses Deep Neural Networks.*
- **Amphion: An Open-Source Audio, Music and Speech Generation Toolkit**, `arXiv, 2312.09911`, [arxiv](http://arxiv.org/abs/2312.09911v1), [pdf](http://arxiv.org/pdf/2312.09911v1.pdf), cication: [**-1**](None)

	 *Xueyao Zhang, Liumeng Xue, Yuancheng Wang, Yicheng Gu, Xi Chen, Zihao Fang, Haopeng Chen, Lexiao Zou, Chaoren Wang, Jun Han* ¬∑ ([huggingface](https://huggingface.co/amphion)) ¬∑ ([Amphion](https://github.com/open-mmlab/Amphion/tree/main) - open-mmlab) ![Star](https://img.shields.io/github/stars/open-mmlab/Amphion.svg?style=social&label=Star)
- [**resemble-enhance**](https://github.com/resemble-ai/resemble-enhance) - resemble-ai ![Star](https://img.shields.io/github/stars/resemble-ai/resemble-enhance.svg?style=social&label=Star)

	 *AI powered speech denoising and enhancement*
- [**awesome-python**](https://github.com/vinta/awesome-python#audio) - vinta ![Star](https://img.shields.io/github/stars/vinta/awesome-python.svg?style=social&label=Star)

	 *A curated list of awesome Python frameworks, libraries, software and resources*
- [**pyannote-audio**](https://github.com/pyannote/pyannote-audio) - pyannote ![Star](https://img.shields.io/github/stars/pyannote/pyannote-audio.svg?style=social&label=Star)

	 *Neural building blocks for speaker diarization: speech activity detection, speaker change detection, overlapped speech detection, speaker embedding*
- [**audiocraft**](https://github.com/facebookresearch/audiocraft) - facebookresearch ![Star](https://img.shields.io/github/stars/facebookresearch/audiocraft.svg?style=social&label=Star)

	 *Audiocraft is a library for audio processing and generation with deep learning. It features the state-of-the-art EnCodec audio compressor / tokenizer, along with MusicGen, a simple and controllable music generation LM with textual and melodic conditioning.*
- [**audio-slicer**](https://github.com/openvpi/audio-slicer) - openvpi ![Star](https://img.shields.io/github/stars/openvpi/audio-slicer.svg?style=social&label=Star)

	 *Python script that slices audio with silence detection*
- [**autocut**](https://github.com/mli/autocut/blob/main/autocut/transcribe.py) - mli ![Star](https://img.shields.io/github/stars/mli/autocut.svg?style=social&label=Star)

## Dataset
- **An Automated End-to-End Open-Source Software for High-Quality
  Text-to-Speech Dataset Generation**, `arXiv, 2402.16380`, [arxiv](http://arxiv.org/abs/2402.16380v1), [pdf](http://arxiv.org/pdf/2402.16380v1.pdf), cication: [**-1**](None)

	 *Ahmet Gunduz, Kamer Ali Yuksel, Kareem Darwish, Golara Javadi, Fabio Minazzi, Nicola Sobieski, Sebastien Bratieres*
- [**common_voice_16_0**](https://huggingface.co/datasets/mozilla-foundation/common_voice_16_0) - mozilla-foundation ü§ó
- [**GigaSpeech**](https://github.com/SpeechColab/GigaSpeech) - SpeechColab ![Star](https://img.shields.io/github/stars/SpeechColab/GigaSpeech.svg?style=social&label=Star)

	 *Large, modern dataset for speech recognition*
![](media/Pasted%20image%2020230726185138.png)
- [‰∏≠Ëã±ÊñáÊï∞ÊçÆÊî∂ÈõÜ](https://yqli.tech/page/data.html)

### TTS
- **LibriTTS-R: A Restored Multi-Speaker Text-to-Speech Corpus**, `arXiv, 2305.18802`, [arxiv](http://arxiv.org/abs/2305.18802v1), [pdf](http://arxiv.org/pdf/2305.18802v1.pdf), cication: [**-1**](None)

	 *Yuma Koizumi, Heiga Zen, Shigeki Karita, Yifan Ding, Kohei Yatabe, Nobuyuki Morioka, Michiel Bacchiani, Yu Zhang, Wei Han, Ankur Bapna* ¬∑ ([openslr](http://www.openslr.org/141/)) ¬∑ ([google.github](https://google.github.io/df-conformer/librittsr/))
- **MLS: A Large-Scale Multilingual Dataset for Speech Research**, `arXiv, 2012.03411`, [arxiv](http://arxiv.org/abs/2012.03411v2), [pdf](http://arxiv.org/pdf/2012.03411v2.pdf), cication: [**-1**](None)

	 *Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, Ronan Collobert* ¬∑ ([openslr](http://www.openslr.org/94/))
	- Multilingual LibriSpeech (MLS) dataset is a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of 8 languages - English, German, Dutch, Spanish, French, Italian, Portuguese, Polish. 
	- segment the audio files into 10-20 second segments 
- **LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech**, `arXiv, 1904.02882`, [arxiv](http://arxiv.org/abs/1904.02882v1), [pdf](http://arxiv.org/pdf/1904.02882v1.pdf), cication: [**-1**](None)

	 *Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J. Weiss, Ye Jia, Zhifeng Chen, Yonghui Wu*
- **Hi-Fi Multi-Speaker English TTS Dataset**, `arXiv, 2104.01497`, [arxiv](http://arxiv.org/abs/2104.01497v3), [pdf](http://arxiv.org/pdf/2104.01497v3.pdf), cication: [**-1**](None)

	 *Evelina Bakhturina, Vitaly Lavrukhin, Boris Ginsburg, Yang Zhang* ¬∑ ([openslr](https://www.openslr.org/109/))

## Audio Techs
- **EfficientSpeech: An On-Device Text to Speech Model**, `arXiv, 2305.13905`, [arxiv](http://arxiv.org/abs/2305.13905v1), [pdf](http://arxiv.org/pdf/2305.13905v1.pdf), cication: [**-1**](None)

	 *Rowel Atienza*
- **Visual-Aware Text-to-Speech**, `arXiv, 2306.12020`, [arxiv](http://arxiv.org/abs/2306.12020v1), [pdf](http://arxiv.org/pdf/2306.12020v1.pdf), cication: [**-1**](None)

	 *Mohan Zhou, Yalong Bai, Wei Zhang, Ting Yao, Tiejun Zhao, Tao Mei*
- **Proactive Detection of Voice Cloning with Localized Watermarking**, `arXiv, 2401.17264`, [arxiv](http://arxiv.org/abs/2401.17264v1), [pdf](http://arxiv.org/pdf/2401.17264v1.pdf), cication: [**-1**](None)

	 *Robin San Roman, Pierre Fernandez, Alexandre D√©fossez, Teddy Furon, Tuan Tran, Hady Elsahar*
- **FADI-AEC: Fast Score Based Diffusion Model Guided by Far-end Signal for
  Acoustic Echo Cancellation**, `arXiv, 2401.04283`, [arxiv](http://arxiv.org/abs/2401.04283v1), [pdf](http://arxiv.org/pdf/2401.04283v1.pdf), cication: [**-1**](None)

	 *Yang Liu, Li Wan, Yun Li, Yiteng Huang, Ming Sun, James Luan, Yangyang Shi, Xin Lei*
- [**AudioSep**](https://github.com/Audio-AGI/AudioSep) - Audio-AGI ![Star](https://img.shields.io/github/stars/Audio-AGI/AudioSep.svg?style=social&label=Star)

	 *Official implementation of "Separate Anything You Describe"*
- **AudioSR: Versatile Audio Super-resolution at Scale**, `arXiv, 2309.07314`, [arxiv](http://arxiv.org/abs/2309.07314v1), [pdf](http://arxiv.org/pdf/2309.07314v1.pdf), cication: [**-1**](None)

	 *Haohe Liu, Ke Chen, Qiao Tian, Wenwu Wang, Mark D. Plumbley*

## Audio Visual
- **Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion
  Latent Aligners**, `arXiv, 2402.17723`, [arxiv](http://arxiv.org/abs/2402.17723v1), [pdf](http://arxiv.org/pdf/2402.17723v1.pdf), cication: [**-1**](None)

	 *Yazhou Xing, Yingqing He, Zeyue Tian, Xintao Wang, Qifeng Chen* ¬∑ ([yzxing87.github](https://yzxing87.github.io/Seeing-and-Hearing/))
- [**vsp-llm**](https://github.com/sally-sh/vsp-llm) - sally-sh ![Star](https://img.shields.io/github/stars/sally-sh/vsp-llm.svg?style=social&label=Star)