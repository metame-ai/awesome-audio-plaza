# Awesome Zero Shot TTS

- [Awesome Zero Shot TTS](#awesome-zero-shot-tts)
  - [Gallery](#gallery)
  - [References](#references)


## Gallery
- [[2402.09378] MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech](https://arxiv.org/abs/2402.09378)

	 ¬∑ ([mobilespeech.github](https://mobilespeech.github.io/))
- [**metavoice-src**](https://github.com/metavoiceio/metavoice-src) - metavoiceio ![Star](https://img.shields.io/github/stars/metavoiceio/metavoice-src.svg?style=social&label=Star)

	 *AI for human-level speech intelligence* ¬∑ ([huggingface](https://huggingface.co/metavoiceio/metavoice-1B-v0.1)) ¬∑ ([ttsdemo.themetavoice](https://ttsdemo.themetavoice.xyz/))
- [**WhisperSpeech**](https://github.com/collabora/WhisperSpeech) - collabora ![Star](https://img.shields.io/github/stars/collabora/WhisperSpeech.svg?style=social&label=Star)

	 *An Open Source text-to-speech system built by inverting Whisper.* ¬∑ ([collabora.github](https://collabora.github.io/WhisperSpeech/))
- **VALL-T: Decoder-Only Generative Transducer for Robust and
  Decoding-Controllable Text-to-Speech**, `arXiv, 2401.14321`, [arxiv](http://arxiv.org/abs/2401.14321v4), [pdf](http://arxiv.org/pdf/2401.14321v4.pdf), cication: [**-1**](None)

	 *Chenpeng Du, Yiwei Guo, Hankun Wang, Yifan Yang, Zhikang Niu, Shuai Wang, Hui Zhang, Xie Chen, Kai Yu* ¬∑ ([cpdu.github](https://cpdu.github.io/vallt/))
- **ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided
  Sequence Reordering**, `arXiv, 2401.07333`, [arxiv](http://arxiv.org/abs/2401.07333v1), [pdf](http://arxiv.org/pdf/2401.07333v1.pdf), cication: [**-1**](None)

	 *Yakun Song, Zhuo Chen, Xiaofei Wang, Ziyang Ma, Xie Chen* ¬∑ ([ereboas.github](https://ereboas.github.io/ELLAV/))
- **OpenVoice: Versatile Instant Voice Cloning**, `arXiv, 2312.01479`, [arxiv](http://arxiv.org/abs/2312.01479v5), [pdf](http://arxiv.org/pdf/2312.01479v5.pdf), cication: [**-1**](None)

	 *Zengyi Qin, Wenliang Zhao, Xumin Yu, Xin Sun* ¬∑ ([openvoice](https://github.com/myshell-ai/openvoice) - myshell-ai) ![Star](https://img.shields.io/github/stars/myshell-ai/openvoice.svg?style=social&label=Star)
- **HierSpeech++: Bridging the Gap between Semantic and Acoustic
  Representation of Speech by Hierarchical Variational Inference for Zero-shot
  Speech Synthesis**, `arXiv, 2311.12454`, [arxiv](http://arxiv.org/abs/2311.12454v2), [pdf](http://arxiv.org/pdf/2311.12454v2.pdf), cication: [**-1**](None)

	 *Sang-Hoon Lee, Ha-Yeong Choi, Seung-Bin Kim, Seong-Whan Lee* ¬∑ ([HierSpeechpp](https://github.com/sh-lee-prml/HierSpeechpp) - sh-lee-prml) ![Star](https://img.shields.io/github/stars/sh-lee-prml/HierSpeechpp.svg?style=social&label=Star)
-  [[2308.16692] SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models](https://arxiv.org/abs/2308.16692)

	 ¬∑ ([speechtokenizer](https://github.com/zhangxinfd/speechtokenizer) - zhangxinfd) ![Star](https://img.shields.io/github/stars/zhangxinfd/speechtokenizer.svg?style=social&label=Star)
- [**xtts**](https://huggingface.co/spaces/coqui/xtts) - coqui ü§ó

	 ¬∑ ([huggingface](https://huggingface.co/spaces/coqui/xtts)) ¬∑ ([tts.readthedocs](https://tts.readthedocs.io/en/dev/models/xtts.html))
- [P-Flow: A Fast and Data-Efficient Zero-Shot TTS through Speech Prompting - NVIDIA ADLR](https://research.nvidia.com/labs/adlr/projects/pflow/)

	 ¬∑ ([openreview](https://openreview.net/pdf?id=zNA7u7wtIN))
- **PromptTTS 2: Describing and Generating Voices with Text Prompt**, `arXiv, 2309.02285`, [arxiv](http://arxiv.org/abs/2309.02285v2), [pdf](http://arxiv.org/pdf/2309.02285v2.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=2491055882531395348&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yichong Leng, Zhifang Guo, Kai Shen, Xu Tan, Zeqian Ju, Yanqing Liu, Yufei Liu, Dongchao Yang, Leying Zhang, Kaitao Song* ¬∑ ([speechresearch.github](https://speechresearch.github.io/prompttts2))
- **SpeechX: Neural Codec Language Model as a Versatile Speech Transformer**, `arXiv, 2308.06873`, [arxiv](http://arxiv.org/abs/2308.06873v1), [pdf](http://arxiv.org/pdf/2308.06873v1.pdf), cication: [**10**](https://scholar.google.com/scholar?cites=3146656686281147659&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Xiaofei Wang, Manthan Thakker, Zhuo Chen, Naoyuki Kanda, Sefik Emre Eskimez, Sanyuan Chen, Min Tang, Shujie Liu, Jinyu Li, Takuya Yoshioka*
- **Mega-TTS 2: Zero-Shot Text-to-Speech with Arbitrary Length Speech
  Prompts**, `arXiv, 2307.07218`, [arxiv](http://arxiv.org/abs/2307.07218v2), [pdf](http://arxiv.org/pdf/2307.07218v2.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=16735322993503076322&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Ziyue Jiang, Jinglin Liu, Yi Ren, Jinzheng He, Chen Zhang, Zhenhui Ye, Pengfei Wei, Chunfeng Wang, Xiang Yin, Zejun Ma* ¬∑ ([mega-tts.github](https://mega-tts.github.io/mega2_demo/))
- [**GPT-SoVITS**](https://github.com/RVC-Boss/GPT-SoVITS?tab=readme-ov-file) - RVC-Boss ![Star](https://img.shields.io/github/stars/RVC-Boss/GPT-SoVITS.svg?style=social&label=Star)

	 *1 min voice data can also be used to train a good TTS model! (few shot voice cloning)*
    - [ËÄóÊó∂‰∏§‰∏™ÊúàËá™‰∏ªÁ†îÂèëÁöÑ‰ΩéÊàêÊú¨AIÈü≥Ëâ≤ÂÖãÈöÜËΩØ‰ª∂ÔºåÂÖçË¥πÈÄÅÁªôÂ§ßÂÆ∂ÔºÅ„ÄêGPT-SoVITS„Äë\_ÂìîÂì©ÂìîÂì©\_bilibili](https://www.bilibili.com/video/BV12g4y1m7Uw/?vd_source=1453a06a1e0b377f5c40946333b4423a)
- [**fish-speech**](https://github.com/fishaudio/fish-speech) - fishaudio ![Star](https://img.shields.io/github/stars/fishaudio/fish-speech.svg?style=social&label=Star)

	 *Brand new TTS solution* ¬∑ ([speech.fish](https://speech.fish.audio/))
- **Pheme: Efficient and Conversational Speech Generation**, `arXiv, 2401.02839`, [arxiv](http://arxiv.org/abs/2401.02839v1), [pdf](http://arxiv.org/pdf/2401.02839v1.pdf), cication: [**-1**](None)

	 *Pawe≈Ç Budzianowski, Taras Sereda, Tomasz Cichy, Ivan Vuliƒá* ¬∑ ([arxiv](https://arxiv.org/pdf/2401.02839.pdf)) ¬∑ ([pheme](https://github.com/PolyAI-LDN/pheme?tab=readme-ov-file) - PolyAI-LDN) ![Star](https://img.shields.io/github/stars/PolyAI-LDN/pheme.svg?style=social&label=Star) ¬∑ ([polyai-ldn.github](https://polyai-ldn.github.io/pheme/#fn1))
- **A Vector Quantized Approach for Text to Speech Synthesis on Real-World
  Spontaneous Speech**, `arXiv, 2302.04215`, [arxiv](http://arxiv.org/abs/2302.04215v1), [pdf](http://arxiv.org/pdf/2302.04215v1.pdf), cication: [**14**](https://scholar.google.com/scholar?cites=1779712084933914260&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Li-Wei Chen, Shinji Watanabe, Alexander Rudnicky*

	 ¬∑ ([MQTTS](https://github.com/b04901014/MQTTS) - b04901014) ![Star](https://img.shields.io/github/stars/b04901014/MQTTS.svg?style=social&label=Star)
- [**SC-CNN**](https://github.com/hcy71o/SC-CNN) - hcy71o ![Star](https://img.shields.io/github/stars/hcy71o/SC-CNN.svg?style=social&label=Star)

	 *SC-CNN: Effective Speaker Conditioning Method for Zero-Shot Multi-Speaker Text-to-Speech Systems*
- [SC-CNN-demo: "Effective Speaker Conditioning Method for Zero-Shot Multi-Speaker Text-to-Speech Systems"](https://hcy71o.github.io/SC-CNN-demo/)
- **Transfer Learning Framework for Low-Resource Text-to-Speech using a
  Large-Scale Unlabeled Speech Corpus**, `arXiv, 2203.15447`, [arxiv](http://arxiv.org/abs/2203.15447v2), [pdf](http://arxiv.org/pdf/2203.15447v2.pdf), cication: [**15**](https://scholar.google.com/scholar?cites=9843370604003434067&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Minchan Kim, Myeonghun Jeong, Byoung Jin Choi, Sunghwan Ahn, Joun Yeop Lee, Nam Soo Kim* ¬∑ ([TransferTTS](https://github.com/hcy71o/TransferTTS) - hcy71o) ![Star](https://img.shields.io/github/stars/hcy71o/TransferTTS.svg?style=social&label=Star) ¬∑ ([SC-VITS](https://github.com/hcy71o/SC-VITS) - hcy71o) ![Star](https://img.shields.io/github/stars/hcy71o/SC-VITS.svg?style=social&label=Star)
- **GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain
  Text-to-Speech**, `arXiv, 2205.07211`, [arxiv](http://arxiv.org/abs/2205.07211v2), [pdf](http://arxiv.org/pdf/2205.07211v2.pdf), cication: [**28**](https://scholar.google.com/scholar?cites=14779318166371088188&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Rongjie Huang, Yi Ren, Jinglin Liu, Chenye Cui, Zhou Zhao* ¬∑ ([generspeech.github](https://generspeech.github.io/#abstract)) ¬∑ ([GenerSpeech](https://github.com/Rongjiehuang/GenerSpeech) - Rongjiehuang) ![Star](https://img.shields.io/github/stars/Rongjiehuang/GenerSpeech.svg?style=social&label=Star)
- **Make-A-Voice: Unified Voice Synthesis With Discrete Representation**, `arXiv, 2305.19269`, [arxiv](http://arxiv.org/abs/2305.19269v1), [pdf](http://arxiv.org/pdf/2305.19269v1.pdf), cication: [**6**](https://scholar.google.com/scholar?cites=12763739973148023166&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Rongjie Huang, Chunlei Zhang, Yongqi Wang, Dongchao Yang, Luping Liu, Zhenhui Ye, Ziyue Jiang, Chao Weng, Zhou Zhao, Dong Yu*
    - [Make-A-Voice](https://make-a-voice.github.io/)
- **Mega-TTS: Zero-Shot Text-to-Speech at Scale with Intrinsic Inductive Bias**, `arXiv, 2306.03509`, [arxiv](http://arxiv.org/abs/2306.03509v1), [pdf](http://arxiv.org/pdf/2306.03509v1.pdf), cication: [**12**](https://scholar.google.com/scholar?cites=15692405188854768212&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Ziyue Jiang, Yi Ren, Zhenhui Ye, Jinglin Liu, Chen Zhang, Qian Yang, Shengpeng Ji, Rongjie Huang, Chunfeng Wang, Xiang Yin*
    - [Mega-TTS | demo-page](https://mega-tts.github.io/demo-page)

- **SoundStorm: Efficient Parallel Audio Generation**, `arXiv, 2305.09636`, [arxiv](http://arxiv.org/abs/2305.09636v1), [pdf](http://arxiv.org/pdf/2305.09636v1.pdf), cication: [**18**](https://scholar.google.com/scholar?cites=12476111044975033127&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Zal√°n Borsos, Matt Sharifi, Damien Vincent, Eugene Kharitonov, Neil Zeghidour, Marco Tagliasacchi*
    - [SoundStorm](https://google-research.github.io/seanet/soundstorm/examples/)
    - [GitHub - lucidrains/soundstorm-pytorch: Implementation of SoundStorm, Efficient Parallel Audio Generation from Google Deepmind, in Pytorch](https://github.com/lucidrains/soundstorm-pytorch)
- **NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot
  Speech and Singing Synthesizers**, `arXiv, 2304.09116`, [arxiv](http://arxiv.org/abs/2304.09116v3), [pdf](http://arxiv.org/pdf/2304.09116v3.pdf), cication: [**43**](https://scholar.google.com/scholar?cites=14054874329026525619&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei He, Tao Qin, Sheng Zhao, Jiang Bian*
    - [GitHub - lucidrains/naturalspeech2-pytorch: Implementation of Natural Speech 2, Zero-shot Speech and Singing Synthesizer, in Pytorch](https://github.com/lucidrains/naturalspeech2-pytorch)
    - [NaturalSpeech2](https://github.com/open-mmlab/Amphion/tree/main/egs/tts/NaturalSpeech2)
    - [NaturalSpeech2 - a Hugging Face Space by amphion](https://huggingface.co/spaces/amphion/NaturalSpeech2)
    - [NaturalSpeech 2](https://speechresearch.github.io/naturalspeech2/)
    - [ÂæÆËΩØ NaturalSpeech 2Êù•‰∫ÜÔºåÂü∫‰∫éÊâ©Êï£Ê®°ÂûãÁöÑËØ≠Èü≥ÂêàÊàê\_ÂìîÂì©ÂìîÂì©\_bilibili](https://www.bilibili.com/video/BV1Cc411P7sZ/?vd_source=1453a06a1e0b377f5c40946333b4423a)
- **Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec
  Language Modeling**, `arXiv, 2303.03926`, [arxiv](http://arxiv.org/abs/2303.03926v1), [pdf](http://arxiv.org/pdf/2303.03926v1.pdf), cication: [**37**](https://scholar.google.com/scholar?cites=7121583945279977636&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Ziqiang Zhang, Long Zhou, Chengyi Wang, Sanyuan Chen, Yu Wu, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li*
    - [GitHub - Plachtaa/VALL-E-X: An open source implementation of Microsoft's VALL-E X zero-shot TTS model. Demo is available in https://plachtaa.github.io](https://github.com/Plachtaa/VALL-E-X)
    - [VALL-E X](https://vallex-demo.github.io/)
- **Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal
  Supervision**, `arXiv, 2302.03540`, [arxiv](http://arxiv.org/abs/2302.03540v1), [pdf](http://arxiv.org/pdf/2302.03540v1.pdf), cication: [**45**](https://scholar.google.com/scholar?cites=15014701949812441701&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Eugene Kharitonov, Damien Vincent, Zal√°n Borsos, Rapha√´l Marinier, Sertan Girgin, Olivier Pietquin, Matt Sharifi, Marco Tagliasacchi, Neil Zeghidour*
    - [SPEAR-TTS](https://google-research.github.io/seanet/speartts/examples/)
    - [GitHub - collabora/spear-tts-pytorch: An unofficial PyTorch implementation of SPEAR-TTS.](https://github.com/collabora/spear-tts-pytorch)
- **Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers**, `arXiv, 2301.02111`, [arxiv](http://arxiv.org/abs/2301.02111v1), [pdf](http://arxiv.org/pdf/2301.02111v1.pdf), cication: [**182**](https://scholar.google.com/scholar?cites=8330191945346870138&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li*
    - [VALL-E](https://vall-e.io/)
    - [GitHub - enhuiz/vall-e: An unofficial PyTorch implementation of the audio LM VALL-E](https://github.com/enhuiz/vall-e)
- [HierSpeech: Bridging the Gap between Text and Speech by Hierarchical Variational Inference using Self-supervised Representations for Speech Synthesis | OpenReview](https://openreview.net/forum?id=awdyRVnfQKX)

	 ¬∑ ([sh-lee-prml.github](https://sh-lee-prml.github.io/hierspeech-demo/)) ¬∑ ([HierSpeech](https://github.com/CODEJIN/HierSpeech) - CODEJIN) ![Star](https://img.shields.io/github/stars/CODEJIN/HierSpeech.svg?style=social&label=Star)
 
- **SNAC: Speaker-normalized affine coupling layer in flow-based
  architecture for zero-shot multi-speaker text-to-speech**, `ieee signal processing letters, 2022`, [arxiv](http://arxiv.org/abs/2211.16866v1), [pdf](http://arxiv.org/pdf/2211.16866v1.pdf), cication: [**7**](https://scholar.google.com/scholar?cites=798443837934930670&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Byoung Jin Choi, Myeonghun Jeong, Joun Yeop Lee, Nam Soo Kim* ¬∑ ([byoungjinchoi.github](https://byoungjinchoi.github.io/snac/))
	 - [GitHub - hcy71o/SNAC](https://github.com/hcy71o/SNAC)

- **YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice
  Conversion for everyone**, `ICML, 2022`, [arxiv](http://arxiv.org/abs/2112.02418v4), [pdf](http://arxiv.org/pdf/2112.02418v4.pdf), cication: [**164**](https://scholar.google.com/scholar?cites=8575580251111777245&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Edresson Casanova, Julian Weber, Christopher Shulby, Arnaldo Candido Junior, Eren G√∂lge, Moacir Antonelli Ponti*
    - [YourTTS](https://edresson.github.io/YourTTS/)


## References
- [**open-tts-tracker**](https://github.com/Vaibhavs10/open-tts-tracker) - Vaibhavs10 ![Star](https://img.shields.io/github/stars/Vaibhavs10/open-tts-tracker.svg?style=social&label=Star)
    - [open_tts_tracker](https://www.reddit.com/r/LocalLLaMA/comments/19fegt5/open_tts_tracker/)
- [AR-NAR-TTS.pdf](https://tan-xu.github.io/AR-NAR-TTS.pdf)
- [„ÄêÊ©üÂô®Â≠∏Áøí2023„ÄëË™ûÈü≥Âü∫Áü≥Ê®°Âûã (Âä©ÊïôÂºµÂá±ÁÇ∫Ë¨õÊéà) (1/2) - YouTube](https://www.youtube.com/watch?v=m7Be7ppR6q0&ab_channel=Hung-yiLee)
- [„ÄêÊ©üÂô®Â≠∏Áøí2023„ÄëË™ûÈü≥Âü∫Áü≥Ê®°Âûã (Âä©ÊïôÂºµÂá±ÁÇ∫Ë¨õÊéà) (2/2) - YouTube](https://www.youtube.com/watch?v=HTAq-CPrU5s&ab_channel=Hung-yiLee)
- [ml2023-course-data/ÂºµÂá±Áà≤-x-Ê©üÂô®Â≠∏Áøí-x-Ë™ûÈü≥Âü∫Áü≥Ê®°Âûã.pdf](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/%E5%BC%B5%E5%87%B1%E7%88%B2-x-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-x-%E8%AA%9E%E9%9F%B3%E5%9F%BA%E7%9F%B3%E6%A8%A1%E5%9E%8B.pdf)