# Awesome Zero Shot TTS

- [Awesome Zero Shot TTS](#awesome-zero-shot-tts)
  - [Gallery](#gallery)
  - [References](#references)

## Gallery
- [2404.05600](https://arxiv.org/pdf/2404.05600)
- [[2409.12139] Takin: A Cohort of Superior Quality Zero-shot Speech Generation Models](https://arxiv.org/abs/2409.12139)

	 · ([takinaudiollm.github](https://takinaudiollm.github.io/))
- [[2409.09351] E1 TTS: Simple and Fast Non-Autoregressive TTS](https://arxiv.org/abs/2409.09351)

	 · ([e1tts.github](https://e1tts.github.io/)) · ([e1tts.github.io](https://github.com/e1tts/e1tts.github.io/) - e1tts) ![Star](https://img.shields.io/github/stars/e1tts/e1tts.github.io.svg?style=social&label=Star)
- [[2409.10058] StyleTTS-ZS: Efficient High-Quality Zero-Shot Text-to-Speech Synthesis with Distilled Time-Varying Style Diffusion](https://arxiv.org/abs/2409.10058)

	 · ([StyleTTS-ZS](https://github.com/yl4579/StyleTTS-ZS) - yl4579) ![Star](https://img.shields.io/github/stars/yl4579/StyleTTS-ZS.svg?style=social&label=Star) · ([styletts-zs.github](https://styletts-zs.github.io/))
- **MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec
  Transformer**, `arXiv, 2409.00750`, [arxiv](http://arxiv.org/abs/2409.00750v1), [pdf](http://arxiv.org/pdf/2409.00750v1.pdf), cication: [**-1**](None)

	 *Yuancheng Wang, Haoyue Zhan, Liwei Liu, Ruihong Zeng, Haotian Guo, Jiachen Zheng, Qiang Zhang, Shunsi Zhang, Zhizheng Wu* · ([maskgct.github](https://maskgct.github.io/))
- **FireRedTTS: A Foundation Text-To-Speech Framework for Industry-Level
  Generative Speech Applications**, `arXiv, 2409.03283`, [arxiv](http://arxiv.org/abs/2409.03283v1), [pdf](http://arxiv.org/pdf/2409.03283v1.pdf), cication: [**-1**](None)

	 *Hao-Han Guo, Kun Liu, Fei-Yu Shen, Yi-Chen Wu, Feng-Long Xie, Kun Xie, Kai-Tuo Xu* · ([fireredteam.github](https://fireredteam.github.io/demos/firered_tts/))
- [Parakeet A natural sounding, conversational text-to-speech model](https://jordandarefsky.com/blog/2024/parakeet/)
- [Bailing-TTS](https://c9412600.github.io/bltts_tech_report/index.html)

	 · ([c9412600.github](https://c9412600.github.io/bltts_tech_report/index.html))
- [**parler-tts**](https://github.com/huggingface/parler-tts/tree/main) - huggingface ![Star](https://img.shields.io/github/stars/huggingface/parler-tts.svg?style=social&label=Star)

	 *Inference and training library for high-quality TTS models.*
- **Laugh Now Cry Later: Controlling Time-Varying Emotional States of
  Flow-Matching-Based Zero-Shot Text-to-Speech**, `arXiv, 2407.12229`, [arxiv](http://arxiv.org/abs/2407.12229v1), [pdf](http://arxiv.org/pdf/2407.12229v1.pdf), cication: [**-1**](None)

	 *Haibin Wu, Xiaofei Wang, Sefik Emre Eskimez, Manthan Thakker, Daniel Tompkins, Chung-Hsien Tsai, Canrun Li, Zhen Xiao, Sheng Zhao, Jinyu Li* · ([aka](https://aka.ms/emoctrl-tts))
- **Autoregressive Speech Synthesis without Vector Quantization**, `arXiv, 2407.08551`, [arxiv](http://arxiv.org/abs/2407.08551v1), [pdf](http://arxiv.org/pdf/2407.08551v1.pdf), cication: [**-1**](None)

	 *Lingwei Meng, Long Zhou, Shujie Liu, Sanyuan Chen, Bing Han, Shujie Hu, Yanqing Liu, Jinyu Li, Sheng Zhao, Xixin Wu*

	 · ([aka](https://aka.ms/melle))
- [**CosyVoice**](https://github.com/FunAudioLLM/CosyVoice?tab=readme-ov-file) - FunAudioLLM ![Star](https://img.shields.io/github/stars/FunAudioLLM/CosyVoice.svg?style=social&label=Star)

	 *Multi-lingual large voice generation model, providing inference, training and deployment full-stack ability.* · ([fun-audio-llm.github](https://fun-audio-llm.github.io/pdf/CosyVoice_v1.pdf))
- [Lightweight Zero-shot Text-to-Speech with Mixture of Adapters](https://ntt-hilab-gensp.github.io/is2024lightweightTTS/)
- **Robust Zero-Shot Text-to-Speech Synthesis with Reverse Inference
  Optimization**, `arXiv, 2407.02243`, [arxiv](http://arxiv.org/abs/2407.02243v1), [pdf](http://arxiv.org/pdf/2407.02243v1.pdf), cication: [**-1**](None)

	 *Yuchen Hu, Chen Chen, Siyin Wang, Eng Siong Chng, Chao Zhang*
- **Improving Robustness of LLM-based Speech Synthesis by Learning Monotonic
  Alignment**, `arXiv, 2406.17957`, [arxiv](http://arxiv.org/abs/2406.17957v1), [pdf](http://arxiv.org/pdf/2406.17957v1.pdf), cication: [**-1**](None)

	 *Paarth Neekhara, Shehzeen Hussain, Subhankar Ghosh, Jason Li, Rafael Valle, Rohan Badlani, Boris Ginsburg* · ([t5tts.github](https://t5tts.github.io/))
- **E2 TTS: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS**, `arXiv, 2406.18009`, [arxiv](http://arxiv.org/abs/2406.18009v1), [pdf](http://arxiv.org/pdf/2406.18009v1.pdf), cication: [**-1**](None)

	 *Sefik Emre Eskimez, Xiaofei Wang, Manthan Thakker, Canrun Li, Chung-Hsien Tsai, Zhen Xiao, Hemin Yang, Zirun Zhu, Min Tang, Xu Tan* · ([microsoft](https://www.microsoft.com/en-us/research/project/e2-tts/))
- **High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer
  and Group Masked Language Model**, `arXiv, 2406.17310`, [arxiv](http://arxiv.org/abs/2406.17310v1), [pdf](http://arxiv.org/pdf/2406.17310v1.pdf), cication: [**-1**](None)

	 *Joun Yeop Lee, Myeonghun Jeong, Minchan Kim, Ji-Hyun Lee, Hoon-Young Cho, Nam Soo Kim* · ([arxiv](https://arxiv.org/pdf/2406.17310))
- **VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via
  Monotonic Alignment**, `arXiv, 2406.07855`, [arxiv](http://arxiv.org/abs/2406.07855v1), [pdf](http://arxiv.org/pdf/2406.07855v1.pdf), cication: [**-1**](None)

	 *Bing Han, Long Zhou, Shujie Liu, Sanyuan Chen, Lingwei Meng, Yanming Qian, Yanqing Liu, Sheng Zhao, Jinyu Li, Furu Wei*
- [**MARS5-TTS**](https://github.com/Camb-ai/MARS5-TTS/tree/master) - Camb-ai ![Star](https://img.shields.io/github/stars/Camb-ai/MARS5-TTS.svg?style=social&label=Star)

	 *MARS5 speech model (TTS) from CAMB.AI*
- **Towards Expressive Zero-Shot Speech Synthesis with Hierarchical Prosody
  Modeling**, `arXiv, 2406.05681`, [arxiv](http://arxiv.org/abs/2406.05681v2), [pdf](http://arxiv.org/pdf/2406.05681v2.pdf), cication: [**-1**](None)

	 *Yuepeng Jiang, Tao Li, Fengyu Yang, Lei Xie, Meng Meng, Yujun Wang*
- **XTTS: a Massively Multilingual Zero-Shot Text-to-Speech Model**, `arXiv, 2406.04904`, [arxiv](http://arxiv.org/abs/2406.04904v1), [pdf](http://arxiv.org/pdf/2406.04904v1.pdf), cication: [**-1**](None)

	 *Edresson Casanova, Kelly Davis, Eren Gölge, Görkem Göknar, Iulian Gulea, Logan Hart, Aya Aljafari, Joshua Meyer, Reuben Morais, Samuel Olayemi*
- **VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text
  to Speech Synthesizers**, `arXiv, 2406.05370`, [arxiv](http://arxiv.org/abs/2406.05370v1), [pdf](http://arxiv.org/pdf/2406.05370v1.pdf), cication: [**-1**](None)

	 *Sanyuan Chen, Shujie Liu, Long Zhou, Yanqing Liu, Xu Tan, Jinyu Li, Sheng Zhao, Yao Qian, Furu Wei* · ([aka](https://aka.ms/valle2))
- **LiveSpeech: Low-Latency Zero-shot Text-to-Speech via Autoregressive
  Modeling of Audio Discrete Codes**, `arXiv, 2406.02897`, [arxiv](http://arxiv.org/abs/2406.02897v1), [pdf](http://arxiv.org/pdf/2406.02897v1.pdf), cication: [**-1**](None)

	 *Trung Dang, David Aponte, Dung Tran, Kazuhito Koishida*
- **Generative Pre-trained Speech Language Model with Efficient Hierarchical
  Transformer**, `arXiv, 2406.00976`, [arxiv](http://arxiv.org/abs/2406.00976v1), [pdf](http://arxiv.org/pdf/2406.00976v1.pdf), cication: [**-1**](None)

	 *Yongxin Zhu, Dan Su, Liqiang He, Linli Xu, Dong Yu* · ([youngsheen.github](https://youngsheen.github.io/GPST/demo/)) · ([GPST](https://github.com/youngsheen/GPST) - youngsheen) ![Star](https://img.shields.io/github/stars/youngsheen/GPST.svg?style=social&label=Star)
- **ControlSpeech: Towards Simultaneous Zero-shot Speaker Cloning and
  Zero-shot Language Style Control With Decoupled Codec**, `arXiv, 2406.01205`, [arxiv](http://arxiv.org/abs/2406.01205v1), [pdf](http://arxiv.org/pdf/2406.01205v1.pdf), cication: [**-1**](None)

	 *Shengpeng Ji, Jialong Zuo, Minghui Fang, Siqi Zheng, Qian Chen, Wen Wang, Ziyue Jiang, Hai Huang, Xize Cheng, Rongjie Huang* · ([ControlSpeech](https://github.com/jishengpeng/ControlSpeech) - jishengpeng) ![Star](https://img.shields.io/github/stars/jishengpeng/ControlSpeech.svg?style=social&label=Star) · ([controlspeech.github](https://controlspeech.github.io/))
- **Seed-TTS: A Family of High-Quality Versatile Speech Generation Models**, `arXiv, 2406.02430`, [arxiv](http://arxiv.org/abs/2406.02430v1), [pdf](http://arxiv.org/pdf/2406.02430v1.pdf), cication: [**-1**](None)

	 *Philip Anastassiou, Jiawei Chen, Jitong Chen, Yuanzhe Chen, Zhuo Chen, Ziyi Chen, Jian Cong, Lelai Deng, Chuang Ding, Lu Gao* · ([bytedancespeech.github](https://bytedancespeech.github.io/seedtts_tech_report/))

	 · ([seed-tts-eval](https://github.com/BytedanceSpeech/seed-tts-eval) - BytedanceSpeech) ![Star](https://img.shields.io/github/stars/BytedanceSpeech/seed-tts-eval.svg?style=social&label=Star)
- **FlashSpeech: Efficient Zero-Shot Speech Synthesis**, `arXiv, 2404.14700`, [arxiv](http://arxiv.org/abs/2404.14700v1), [pdf](http://arxiv.org/pdf/2404.14700v1.pdf), cication: [**-1**](None)

	 *Zhen Ye, Zeqian Ju, Haohe Liu, Xu Tan, Jianyi Chen, Yiwen Lu, Peiwen Sun, Jiahao Pan, Weizhen Bian, Shulin He* · ([flashspeech.github](https://flashspeech.github.io/))
- **RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting
  for Text-to-Speech Synthesis**, `arXiv, 2404.03204`, [arxiv](http://arxiv.org/abs/2404.03204v1), [pdf](http://arxiv.org/pdf/2404.03204v1.pdf), cication: [**-1**](None)

	 *Detai Xin, Xu Tan, Kai Shen, Zeqian Ju, Dongchao Yang, Yuancheng Wang, Shinnosuke Takamichi, Hiroshi Saruwatari, Shujie Liu, Jinyu Li* · ([ralle-demo.github](https://ralle-demo.github.io/RALL-E/))
- **VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild**, `arXiv, 2403.16973`, [arxiv](http://arxiv.org/abs/2403.16973v1), [pdf](http://arxiv.org/pdf/2403.16973v1.pdf), cication: [**-1**](None)

	 *Puyuan Peng, Po-Yao Huang, Daniel Li, Abdelrahman Mohamed, David Harwath*

	 · ([jasonppy.github](https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf))
	 · ([VoiceCraft](https://github.com/jasonppy/VoiceCraft) - jasonppy) ![Star](https://img.shields.io/github/stars/jasonppy/VoiceCraft.svg?style=social&label=Star) · ([jasonppy.github](https://jasonppy.github.io/VoiceCraft_web/))
- **Mega-TTS 2: Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis**, `arXiv, 2307.07218`, [arxiv](http://arxiv.org/abs/2307.07218v4), [pdf](http://arxiv.org/pdf/2307.07218v4.pdf), cication: [**-1**](None)

	 *Ziyue Jiang, Jinglin Liu, Yi Ren, Jinzheng He, Zhenhui Ye, Shengpeng Ji, Qian Yang, Chen Zhang, Pengfei Wei, Chunfeng Wang*

	 · ([boostprompt.github](https://boostprompt.github.io/boostprompt/))
- [CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech | OpenReview](https://openreview.net/forum?id=ofzeypWosV)

	 · ([scholar-inbox](https://www.scholar-inbox.com/papers/Kim2024ICLR_CLaM_TTS_Improving_Neural.pdf)) · ([clam-tts.github](https://clam-tts.github.io/))
- **HAM-TTS: Hierarchical Acoustic Modeling for Token-Based Zero-Shot
  Text-to-Speech with Model and Data Scaling**, `arXiv, 2403.05989`, [arxiv](http://arxiv.org/abs/2403.05989v1), [pdf](http://arxiv.org/pdf/2403.05989v1.pdf), cication: [**-1**](None)

	 *Chunhui Wang, Chang Zeng, Bowen Zhang, Ziyang Ma, Yefan Zhu, Zifeng Cai, Jian Zhao, Zhonglin Jiang, Yong Chen* · ([anonymous.4open](https://anonymous.4open.science/w/ham-tts/))
- **NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and
  Diffusion Models**, `arXiv, 2403.03100`, [arxiv](http://arxiv.org/abs/2403.03100v1), [pdf](http://arxiv.org/pdf/2403.03100v1.pdf), cication: [**-1**](None)

	 *Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang, Yanqing Liu, Yichong Leng, Kaitao Song, Siliang Tang*

	 · ([speechresearch.github](https://speechresearch.github.io/naturalspeech3/))
- **MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot
  Text-to-Speech**, `arXiv, 2402.09378`, [arxiv](http://arxiv.org/abs/2402.09378v1), [pdf](http://arxiv.org/pdf/2402.09378v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=15185926768577512217&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Shengpeng Ji, Ziyue Jiang, Hanting Wang, Jialong Zuo, Zhou Zhao*

	 · ([mobilespeech.github](https://mobilespeech.github.io/))
- [**metavoice-src**](https://github.com/metavoiceio/metavoice-src) - metavoiceio ![Star](https://img.shields.io/github/stars/metavoiceio/metavoice-src.svg?style=social&label=Star)

	 *AI for human-level speech intelligence* · ([huggingface](https://huggingface.co/metavoiceio/metavoice-1B-v0.1)) · ([ttsdemo.themetavoice](https://ttsdemo.themetavoice.xyz/))
- [**WhisperSpeech**](https://github.com/collabora/WhisperSpeech) - collabora ![Star](https://img.shields.io/github/stars/collabora/WhisperSpeech.svg?style=social&label=Star)

	 *An Open Source text-to-speech system built by inverting Whisper.* · ([collabora.github](https://collabora.github.io/WhisperSpeech/))
- **VALL-T: Decoder-Only Generative Transducer for Robust and
  Decoding-Controllable Text-to-Speech**, `arXiv, 2401.14321`, [arxiv](http://arxiv.org/abs/2401.14321v4), [pdf](http://arxiv.org/pdf/2401.14321v4.pdf), cication: [**-1**](None)

	 *Chenpeng Du, Yiwei Guo, Hankun Wang, Yifan Yang, Zhikang Niu, Shuai Wang, Hui Zhang, Xie Chen, Kai Yu* · ([cpdu.github](https://cpdu.github.io/vallt/))
- **ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided
  Sequence Reordering**, `arXiv, 2401.07333`, [arxiv](http://arxiv.org/abs/2401.07333v1), [pdf](http://arxiv.org/pdf/2401.07333v1.pdf), cication: [**-1**](None)

	 *Yakun Song, Zhuo Chen, Xiaofei Wang, Ziyang Ma, Xie Chen* · ([ereboas.github](https://ereboas.github.io/ELLAV/))
- **OpenVoice: Versatile Instant Voice Cloning**, `arXiv, 2312.01479`, [arxiv](http://arxiv.org/abs/2312.01479v5), [pdf](http://arxiv.org/pdf/2312.01479v5.pdf), cication: [**-1**](None)

	 *Zengyi Qin, Wenliang Zhao, Xumin Yu, Xin Sun* · ([openvoice](https://github.com/myshell-ai/openvoice) - myshell-ai) ![Star](https://img.shields.io/github/stars/myshell-ai/openvoice.svg?style=social&label=Star)
- **HierSpeech++: Bridging the Gap between Semantic and Acoustic
  Representation of Speech by Hierarchical Variational Inference for Zero-shot
  Speech Synthesis**, `arXiv, 2311.12454`, [arxiv](http://arxiv.org/abs/2311.12454v2), [pdf](http://arxiv.org/pdf/2311.12454v2.pdf), cication: [**-1**](None)

	 *Sang-Hoon Lee, Ha-Yeong Choi, Seung-Bin Kim, Seong-Whan Lee* · ([HierSpeechpp](https://github.com/sh-lee-prml/HierSpeechpp) - sh-lee-prml) ![Star](https://img.shields.io/github/stars/sh-lee-prml/HierSpeechpp.svg?style=social&label=Star)
	 
- **SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language
  Models**, `arXiv, 2308.16692`, [arxiv](http://arxiv.org/abs/2308.16692v2), [pdf](http://arxiv.org/pdf/2308.16692v2.pdf), cication: [**13**](https://scholar.google.com/scholar?cites=10604890170283513348&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, Xipeng Qiu*

	 · ([speechtokenizer](https://github.com/zhangxinfd/speechtokenizer) - zhangxinfd) ![Star](https://img.shields.io/github/stars/zhangxinfd/speechtokenizer.svg?style=social&label=Star)
- [**xtts**](https://huggingface.co/spaces/coqui/xtts) - coqui 🤗

	 · ([huggingface](https://huggingface.co/spaces/coqui/xtts)) · ([tts.readthedocs](https://tts.readthedocs.io/en/dev/models/xtts.html))
- [P-Flow: A Fast and Data-Efficient Zero-Shot TTS through Speech Prompting - NVIDIA ADLR](https://research.nvidia.com/labs/adlr/projects/pflow/)

	 · ([openreview](https://openreview.net/pdf?id=zNA7u7wtIN))
- **PromptTTS 2: Describing and Generating Voices with Text Prompt**, `arXiv, 2309.02285`, [arxiv](http://arxiv.org/abs/2309.02285v2), [pdf](http://arxiv.org/pdf/2309.02285v2.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=2491055882531395348&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yichong Leng, Zhifang Guo, Kai Shen, Xu Tan, Zeqian Ju, Yanqing Liu, Yufei Liu, Dongchao Yang, Leying Zhang, Kaitao Song* · ([speechresearch.github](https://speechresearch.github.io/prompttts2))
- **SpeechX: Neural Codec Language Model as a Versatile Speech Transformer**, `arXiv, 2308.06873`, [arxiv](http://arxiv.org/abs/2308.06873v1), [pdf](http://arxiv.org/pdf/2308.06873v1.pdf), cication: [**10**](https://scholar.google.com/scholar?cites=3146656686281147659&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Xiaofei Wang, Manthan Thakker, Zhuo Chen, Naoyuki Kanda, Sefik Emre Eskimez, Sanyuan Chen, Min Tang, Shujie Liu, Jinyu Li, Takuya Yoshioka*
- **Mega-TTS 2: Zero-Shot Text-to-Speech with Arbitrary Length Speech
  Prompts**, `arXiv, 2307.07218`, [arxiv](http://arxiv.org/abs/2307.07218v2), [pdf](http://arxiv.org/pdf/2307.07218v2.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=16735322993503076322&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Ziyue Jiang, Jinglin Liu, Yi Ren, Jinzheng He, Chen Zhang, Zhenhui Ye, Pengfei Wei, Chunfeng Wang, Xiang Yin, Zejun Ma* · ([mega-tts.github](https://mega-tts.github.io/mega2_demo/))
- [**GPT-SoVITS**](https://github.com/RVC-Boss/GPT-SoVITS?tab=readme-ov-file) - RVC-Boss ![Star](https://img.shields.io/github/stars/RVC-Boss/GPT-SoVITS.svg?style=social&label=Star)

	 *1 min voice data can also be used to train a good TTS model! (few shot voice cloning)*
    - [耗时两个月自主研发的低成本AI音色克隆软件，免费送给大家！【GPT-SoVITS】\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV12g4y1m7Uw/?vd_source=1453a06a1e0b377f5c40946333b4423a)
- [**fish-speech**](https://github.com/fishaudio/fish-speech) - fishaudio ![Star](https://img.shields.io/github/stars/fishaudio/fish-speech.svg?style=social&label=Star)

	 *Brand new TTS solution* · ([speech.fish](https://speech.fish.audio/))

	 · ([bilibili](https://www.bilibili.com/video/BV1zJ4m1K7cj))
- **Pheme: Efficient and Conversational Speech Generation**, `arXiv, 2401.02839`, [arxiv](http://arxiv.org/abs/2401.02839v1), [pdf](http://arxiv.org/pdf/2401.02839v1.pdf), cication: [**-1**](None)

	 *Paweł Budzianowski, Taras Sereda, Tomasz Cichy, Ivan Vulić* · ([arxiv](https://arxiv.org/pdf/2401.02839.pdf)) · ([pheme](https://github.com/PolyAI-LDN/pheme?tab=readme-ov-file) - PolyAI-LDN) ![Star](https://img.shields.io/github/stars/PolyAI-LDN/pheme.svg?style=social&label=Star) · ([polyai-ldn.github](https://polyai-ldn.github.io/pheme/#fn1))
- **A Vector Quantized Approach for Text to Speech Synthesis on Real-World
  Spontaneous Speech**, `arXiv, 2302.04215`, [arxiv](http://arxiv.org/abs/2302.04215v1), [pdf](http://arxiv.org/pdf/2302.04215v1.pdf), cication: [**14**](https://scholar.google.com/scholar?cites=1779712084933914260&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Li-Wei Chen, Shinji Watanabe, Alexander Rudnicky*

	 · ([MQTTS](https://github.com/b04901014/MQTTS) - b04901014) ![Star](https://img.shields.io/github/stars/b04901014/MQTTS.svg?style=social&label=Star)
- [**SC-CNN**](https://github.com/hcy71o/SC-CNN) - hcy71o ![Star](https://img.shields.io/github/stars/hcy71o/SC-CNN.svg?style=social&label=Star)

	 *SC-CNN: Effective Speaker Conditioning Method for Zero-Shot Multi-Speaker Text-to-Speech Systems*
- [SC-CNN-demo: "Effective Speaker Conditioning Method for Zero-Shot Multi-Speaker Text-to-Speech Systems"](https://hcy71o.github.io/SC-CNN-demo/)
- **Transfer Learning Framework for Low-Resource Text-to-Speech using a
  Large-Scale Unlabeled Speech Corpus**, `arXiv, 2203.15447`, [arxiv](http://arxiv.org/abs/2203.15447v2), [pdf](http://arxiv.org/pdf/2203.15447v2.pdf), cication: [**15**](https://scholar.google.com/scholar?cites=9843370604003434067&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Minchan Kim, Myeonghun Jeong, Byoung Jin Choi, Sunghwan Ahn, Joun Yeop Lee, Nam Soo Kim* · ([TransferTTS](https://github.com/hcy71o/TransferTTS) - hcy71o) ![Star](https://img.shields.io/github/stars/hcy71o/TransferTTS.svg?style=social&label=Star) · ([SC-VITS](https://github.com/hcy71o/SC-VITS) - hcy71o) ![Star](https://img.shields.io/github/stars/hcy71o/SC-VITS.svg?style=social&label=Star)
- **GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain
  Text-to-Speech**, `arXiv, 2205.07211`, [arxiv](http://arxiv.org/abs/2205.07211v2), [pdf](http://arxiv.org/pdf/2205.07211v2.pdf), cication: [**28**](https://scholar.google.com/scholar?cites=14779318166371088188&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Rongjie Huang, Yi Ren, Jinglin Liu, Chenye Cui, Zhou Zhao* · ([generspeech.github](https://generspeech.github.io/#abstract)) · ([GenerSpeech](https://github.com/Rongjiehuang/GenerSpeech) - Rongjiehuang) ![Star](https://img.shields.io/github/stars/Rongjiehuang/GenerSpeech.svg?style=social&label=Star)
- **Make-A-Voice: Unified Voice Synthesis With Discrete Representation**, `arXiv, 2305.19269`, [arxiv](http://arxiv.org/abs/2305.19269v1), [pdf](http://arxiv.org/pdf/2305.19269v1.pdf), cication: [**6**](https://scholar.google.com/scholar?cites=12763739973148023166&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Rongjie Huang, Chunlei Zhang, Yongqi Wang, Dongchao Yang, Luping Liu, Zhenhui Ye, Ziyue Jiang, Chao Weng, Zhou Zhao, Dong Yu*
    - [Make-A-Voice](https://make-a-voice.github.io/)
- **Mega-TTS: Zero-Shot Text-to-Speech at Scale with Intrinsic Inductive Bias**, `arXiv, 2306.03509`, [arxiv](http://arxiv.org/abs/2306.03509v1), [pdf](http://arxiv.org/pdf/2306.03509v1.pdf), cication: [**12**](https://scholar.google.com/scholar?cites=15692405188854768212&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Ziyue Jiang, Yi Ren, Zhenhui Ye, Jinglin Liu, Chen Zhang, Qian Yang, Shengpeng Ji, Rongjie Huang, Chunfeng Wang, Xiang Yin*
    - [Mega-TTS | demo-page](https://mega-tts.github.io/demo-page)

- **SoundStorm: Efficient Parallel Audio Generation**, `arXiv, 2305.09636`, [arxiv](http://arxiv.org/abs/2305.09636v1), [pdf](http://arxiv.org/pdf/2305.09636v1.pdf), cication: [**18**](https://scholar.google.com/scholar?cites=12476111044975033127&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Zalán Borsos, Matt Sharifi, Damien Vincent, Eugene Kharitonov, Neil Zeghidour, Marco Tagliasacchi*
    - [SoundStorm](https://google-research.github.io/seanet/soundstorm/examples/)
    - [GitHub - lucidrains/soundstorm-pytorch: Implementation of SoundStorm, Efficient Parallel Audio Generation from Google Deepmind, in Pytorch](https://github.com/lucidrains/soundstorm-pytorch)
- **NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot
  Speech and Singing Synthesizers**, `arXiv, 2304.09116`, [arxiv](http://arxiv.org/abs/2304.09116v3), [pdf](http://arxiv.org/pdf/2304.09116v3.pdf), cication: [**43**](https://scholar.google.com/scholar?cites=14054874329026525619&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei He, Tao Qin, Sheng Zhao, Jiang Bian*
    - [GitHub - lucidrains/naturalspeech2-pytorch: Implementation of Natural Speech 2, Zero-shot Speech and Singing Synthesizer, in Pytorch](https://github.com/lucidrains/naturalspeech2-pytorch)
    - [NaturalSpeech2](https://github.com/open-mmlab/Amphion/tree/main/egs/tts/NaturalSpeech2)
    - [NaturalSpeech2 - a Hugging Face Space by amphion](https://huggingface.co/spaces/amphion/NaturalSpeech2)
    - [NaturalSpeech 2](https://speechresearch.github.io/naturalspeech2/)
    - [微软 NaturalSpeech 2来了，基于扩散模型的语音合成\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1Cc411P7sZ/?vd_source=1453a06a1e0b377f5c40946333b4423a)
- **Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec
  Language Modeling**, `arXiv, 2303.03926`, [arxiv](http://arxiv.org/abs/2303.03926v1), [pdf](http://arxiv.org/pdf/2303.03926v1.pdf), cication: [**37**](https://scholar.google.com/scholar?cites=7121583945279977636&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Ziqiang Zhang, Long Zhou, Chengyi Wang, Sanyuan Chen, Yu Wu, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li*
    - [GitHub - Plachtaa/VALL-E-X: An open source implementation of Microsoft's VALL-E X zero-shot TTS model. Demo is available in https://plachtaa.github.io](https://github.com/Plachtaa/VALL-E-X)
    - [VALL-E X](https://vallex-demo.github.io/)
- **Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal
  Supervision**, `arXiv, 2302.03540`, [arxiv](http://arxiv.org/abs/2302.03540v1), [pdf](http://arxiv.org/pdf/2302.03540v1.pdf), cication: [**45**](https://scholar.google.com/scholar?cites=15014701949812441701&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Eugene Kharitonov, Damien Vincent, Zalán Borsos, Raphaël Marinier, Sertan Girgin, Olivier Pietquin, Matt Sharifi, Marco Tagliasacchi, Neil Zeghidour*
    - [SPEAR-TTS](https://google-research.github.io/seanet/speartts/examples/)
    - [GitHub - collabora/spear-tts-pytorch: An unofficial PyTorch implementation of SPEAR-TTS.](https://github.com/collabora/spear-tts-pytorch)
- **Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers**, `arXiv, 2301.02111`, [arxiv](http://arxiv.org/abs/2301.02111v1), [pdf](http://arxiv.org/pdf/2301.02111v1.pdf), cication: [**182**](https://scholar.google.com/scholar?cites=8330191945346870138&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li*
    - [VALL-E](https://vall-e.io/)
    - [GitHub - enhuiz/vall-e: An unofficial PyTorch implementation of the audio LM VALL-E](https://github.com/enhuiz/vall-e)
- [HierSpeech: Bridging the Gap between Text and Speech by Hierarchical Variational Inference using Self-supervised Representations for Speech Synthesis | OpenReview](https://openreview.net/forum?id=awdyRVnfQKX)

	 · ([sh-lee-prml.github](https://sh-lee-prml.github.io/hierspeech-demo/)) · ([HierSpeech](https://github.com/CODEJIN/HierSpeech) - CODEJIN) ![Star](https://img.shields.io/github/stars/CODEJIN/HierSpeech.svg?style=social&label=Star)
 
- **SNAC: Speaker-normalized affine coupling layer in flow-based
  architecture for zero-shot multi-speaker text-to-speech**, `ieee signal processing letters, 2022`, [arxiv](http://arxiv.org/abs/2211.16866v1), [pdf](http://arxiv.org/pdf/2211.16866v1.pdf), cication: [**7**](https://scholar.google.com/scholar?cites=798443837934930670&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Byoung Jin Choi, Myeonghun Jeong, Joun Yeop Lee, Nam Soo Kim* · ([byoungjinchoi.github](https://byoungjinchoi.github.io/snac/))
	 - [GitHub - hcy71o/SNAC](https://github.com/hcy71o/SNAC)

- **YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice
  Conversion for everyone**, `ICML, 2022`, [arxiv](http://arxiv.org/abs/2112.02418v4), [pdf](http://arxiv.org/pdf/2112.02418v4.pdf), cication: [**164**](https://scholar.google.com/scholar?cites=8575580251111777245&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Edresson Casanova, Julian Weber, Christopher Shulby, Arnaldo Candido Junior, Eren Gölge, Moacir Antonelli Ponti*
    - [YourTTS](https://edresson.github.io/YourTTS/)


## Projects
- [**fish-speech-1.4**](https://huggingface.co/fishaudio/fish-speech-1.4) - fishaudio 🤗
- [**ai-voice-cloning**](https://github.com/JarodMica/ai-voice-cloning) - JarodMica ![Star](https://img.shields.io/github/stars/JarodMica/ai-voice-cloning.svg?style=social&label=Star)
- [**Vokan**](https://huggingface.co/ShoukanLabs/Vokan) - ShoukanLabs 🤗

## Products
- [Cartesia](https://play.cartesia.ai/)
- [Introducing Rapid Voice Cloning: Create AI Voices in Seconds](https://www.resemble.ai/introducing-rapid-voice-cloning-create-voice-clones-in-seconds/)

## References
- [**open-tts-tracker**](https://github.com/Vaibhavs10/open-tts-tracker) - Vaibhavs10 ![Star](https://img.shields.io/github/stars/Vaibhavs10/open-tts-tracker.svg?style=social&label=Star)
    - [open_tts_tracker](https://www.reddit.com/r/LocalLLaMA/comments/19fegt5/open_tts_tracker/)
- [AR-NAR-TTS.pdf](https://tan-xu.github.io/AR-NAR-TTS.pdf)
- [【機器學習2023】語音基石模型 (助教張凱為講授) (1/2) - YouTube](https://www.youtube.com/watch?v=m7Be7ppR6q0&ab_channel=Hung-yiLee)
- [【機器學習2023】語音基石模型 (助教張凱為講授) (2/2) - YouTube](https://www.youtube.com/watch?v=HTAq-CPrU5s&ab_channel=Hung-yiLee)
- [ml2023-course-data/張凱爲-x-機器學習-x-語音基石模型.pdf](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/%E5%BC%B5%E5%87%B1%E7%88%B2-x-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-x-%E8%AA%9E%E9%9F%B3%E5%9F%BA%E7%9F%B3%E6%A8%A1%E5%9E%8B.pdf)