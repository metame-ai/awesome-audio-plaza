# Awesome Voice Conversion

- [Awesome Voice Conversion](#awesome-voice-conversion)
  - [Papers](#papers)
  - [Projects](#projects)


## Papers
- **MulliVC: Multi-lingual Voice Conversion With Cycle Consistency**, `arXiv, 2408.04708`, [arxiv](http://arxiv.org/abs/2408.04708v1), [pdf](http://arxiv.org/pdf/2408.04708v1.pdf), cication: [**-1**](None)

	 *Jiawei Huang, Chen Zhang, Yi Ren, Ziyue Jiang, Zhenhui Ye, Jinglin Liu, Jinzheng He, Xiang Yin, Zhou Zhao* Â· ([mullivc.github](https://mullivc.github.io/))
- **VoiceShop: A Unified Speech-to-Speech Framework for Identity-Preserving
  Zero-Shot Voice Editing**, `arXiv, 2404.06674`, [arxiv](http://arxiv.org/abs/2404.06674v2), [pdf](http://arxiv.org/pdf/2404.06674v2.pdf), cication: [**-1**](None)

	 *Philip Anastassiou, Zhenyu Tang, Kainan Peng, Dongya Jia, Jiaxin Li, Ming Tu, Yuping Wang, Yuxuan Wang, Mingbo Ma* Â· ([voiceshopai.github](https://voiceshopai.github.io/))
- **StreamVoice: Streamable Context-Aware Language Modeling for Real-time
  Zero-Shot Voice Conversion**, `arXiv, 2401.11053`, [arxiv](http://arxiv.org/abs/2401.11053v2), [pdf](http://arxiv.org/pdf/2401.11053v2.pdf), cication: [**-1**](None)

	 *Zhichao Wang, Yuanzhe Chen, Xinsheng Wang, Zhuo Chen, Lei Xie, Yuping Wang, Yuxuan Wang*
- [**GPT-SoVITS**](https://github.com/RVC-Boss/GPT-SoVITS) - RVC-Boss ![Star](https://img.shields.io/github/stars/RVC-Boss/GPT-SoVITS.svg?style=social&label=Star)

	 *1 min voice data can also be used to train a good TTS model! (few shot voice cloning)*
- **CoMoSVC: Consistency Model-based Singing Voice Conversion**, `arXiv, 2401.01792`, [arxiv](http://arxiv.org/abs/2401.01792v1), [pdf](http://arxiv.org/pdf/2401.01792v1.pdf), cication: [**-1**](None)

	 *Yiwen Lu, Zhen Ye, Wei Xue, Xu Tan, Qifeng Liu, Yike Guo* Â· ([comosvc.github](https://comosvc.github.io/))
- **Leveraging Content-based Features from Multiple Acoustic Models for
  Singing Voice Conversion**, `arXiv, 2310.11160`, [arxiv](http://arxiv.org/abs/2310.11160v1), [pdf](http://arxiv.org/pdf/2310.11160v1.pdf), cication: [**-1**](None)

	 *Xueyao Zhang, Yicheng Gu, Haopeng Chen, Zihao Fang, Lexiao Zou, Liumeng Xue, Zhizheng Wu* Â· ([zhangxueyao](https://www.zhangxueyao.com/data/MultipleContentsSVC/index.html))
- [**llvc**](https://github.com/koeai/llvc) - koeai ![Star](https://img.shields.io/github/stars/koeai/llvc.svg?style=social&label=Star)
- **Rhythm Modeling for Voice Conversion**, `arXiv, 2307.06040`, [arxiv](http://arxiv.org/abs/2307.06040v1), [pdf](http://arxiv.org/pdf/2307.06040v1.pdf), cication: [**-1**](None)

	 *Benjamin van Niekerk, Marc-AndrÃ© Carbonneau, Herman Kamper*
- **HierVST: Hierarchical Adaptive Zero-shot Voice Style Transfer**, `arXiv, 2307.16171`, [arxiv](http://arxiv.org/abs/2307.16171v1), [pdf](http://arxiv.org/pdf/2307.16171v1.pdf), cication: [**-1**](None)

	 *Sang-Hoon Lee, Ha-Yeong Choi, Hyung-Seok Oh, Seong-Whan Lee*
- **SLMGAN: Exploiting Speech Language Model Representations for
  Unsupervised Zero-Shot Voice Conversion in GANs**, `arXiv, 2307.09435`, [arxiv](http://arxiv.org/abs/2307.09435v1), [pdf](http://arxiv.org/pdf/2307.09435v1.pdf), cication: [**-1**](None)

	 *Yinghao Aaron Li, Cong Han, Nima Mesgarani*
- **Voice Conversion With Just Nearest Neighbors**, `arXiv, 2305.18975`, [arxiv](http://arxiv.org/abs/2305.18975v1), [pdf](http://arxiv.org/pdf/2305.18975v1.pdf), cication: [**-1**](None)

	 *Matthew Baas, Benjamin van Niekerk, Herman Kamper* Â· ([knn-vc](https://github.com/interspeech2023blind/knn-vc) - interspeech2023blind) ![Star](https://img.shields.io/github/stars/interspeech2023blind/knn-vc.svg?style=social&label=Star) Â· ([bshall.github](https://bshall.github.io/knn-vc/))

## Projects
- [**Applio**](https://github.com/IAHispano/Applio) - IAHispano ![Star](https://img.shields.io/github/stars/IAHispano/Applio.svg?style=social&label=Star)

	 *VITS-based Voice Conversion focused on simplicity, quality and performance.*
- [**Retrieval-based-Voice-Conversion-WebUI**](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI) - RVC-Project ![Star](https://img.shields.io/github/stars/RVC-Project/Retrieval-based-Voice-Conversion-WebUI.svg?style=social&label=Star)

	 *Voice data <= 10 mins can also be used to train a good VC model!*

# Voice-Omni

[GitHub - gpt-omni/mini-omni: open-source multimodal large language model that can hear, talk while thinking. Featuring real-time end-to-end speech input and streaming audio output conversational capabilities.](https://github.com/gpt-omni/mini-omni)

## Papers
- **Body of Her: A Preliminary Study on End-to-End Humanoid Agent**, `arXiv, 2408.02879`, [arxiv](http://arxiv.org/abs/2408.02879v1), [pdf](http://arxiv.org/pdf/2408.02879v1.pdf), cication: [**-1**](None)

	 *Tenglong Ao*

	 Â· ([aubrey-ao.github](https://aubrey-ao.github.io/BodyOfHer))
- **VITA: Towards Open-Source Interactive Omni Multimodal LLM**, `arXiv, 2408.05211`, [arxiv](http://arxiv.org/abs/2408.05211v1), [pdf](http://arxiv.org/pdf/2408.05211v1.pdf), cication: [**-1**](None)

	 *Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Meng Zhao, Yifan Zhang, Xiong Wang, Di Yin, Long Ma, Xiawu Zheng* Â· ([vita-home.github](https://vita-home.github.io/)) Â· ([VITA](https://github.com/VITA-MLLM/VITA) - VITA-MLLM) ![Star](https://img.shields.io/github/stars/VITA-MLLM/VITA.svg?style=social&label=Star)
- **Language Model Can Listen While Speaking**, `arXiv, 2408.02622`, [arxiv](http://arxiv.org/abs/2408.02622v1), [pdf](http://arxiv.org/pdf/2408.02622v1.pdf), cication: [**-1**](None)

	 *Ziyang Ma, Yakun Song, Chenpeng Du, Jian Cong, Zhuo Chen, Yuping Wang, Yuxuan Wang, Xie Chen* Â· ([ddlbojack.github](https://ddlbojack.github.io/LSLM))
- **Talk With Human-like Agents: Empathetic Dialogue Through Perceptible
  Acoustic Reception and Reaction**, `arXiv, 2406.12707`, [arxiv](http://arxiv.org/abs/2406.12707v1), [pdf](http://arxiv.org/pdf/2406.12707v1.pdf), cication: [**-1**](None)

	 *Haoqiu Yan, Yongxin Zhu, Kai Zheng, Bing Liu, Haoyu Cao, Deqiang Jiang, Linli Xu* Â· ([PerceptiveAgent](https://github.com/Haoqiu-Yan/PerceptiveAgent) - Haoqiu-Yan) ![Star](https://img.shields.io/github/stars/Haoqiu-Yan/PerceptiveAgent.svg?style=social&label=Star)
- **SpiRit-LM: Interleaved Spoken and Written Language Model**, `arXiv, 2402.05755`, [arxiv](http://arxiv.org/abs/2402.05755v1), [pdf](http://arxiv.org/pdf/2402.05755v1.pdf), cication: [**2**](https://scholar.google.com/scholar?cites=7368374930715826231&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Tu Anh Nguyen, Benjamin Muller, Bokai Yu, Marta R. Costa-jussa, Maha Elbayad, Sravya Popuri, Paul-Ambroise Duquenne, Robin Algayres, Ruslan Mavlyutov, Itai Gat*

	 Â· ([speechbot.github](https://speechbot.github.io/spiritlm/index.html))
- **Speak While You Think: Streaming Speech Synthesis During Text Generation**, `icassp 2024-2024 ieee international conference on acousticsÂ â€¦, 2024`, [arxiv](http://arxiv.org/abs/2309.11210v1), [pdf](http://arxiv.org/pdf/2309.11210v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=2989454037175742919&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Avihu Dekel, Slava Shechtman, Raul Fernandez, David Haws, Zvi Kons, Ron Hoory*
- **LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT**, `arXiv, 2310.04673`, [arxiv](http://arxiv.org/abs/2310.04673v4), [pdf](http://arxiv.org/pdf/2310.04673v4.pdf), cication: [**-1**](None)

	 *Zhihao Du, Jiaming Wang, Qian Chen, Yunfei Chu, Zhifu Gao, Zerui Li, Kai Hu, Xiaohuan Zhou, Jin Xu, Ziyang Ma*
- **VioLA: Unified Codec Language Models for Speech Recognition, Synthesis,
  and Translation**, `arXiv, 2305.16107`, [arxiv](http://arxiv.org/abs/2305.16107v1), [pdf](http://arxiv.org/pdf/2305.16107v1.pdf), cication: [**-1**](None)

	 *Tianrui Wang, Long Zhou, Ziqiang Zhang, Yu Wu, Shujie Liu, Yashesh Gaur, Zhuo Chen, Jinyu Li, Furu Wei*
- **Spoken Question Answering and Speech Continuation Using
  Spectrogram-Powered LLM**, `arXiv, 2305.15255`, [arxiv](http://arxiv.org/abs/2305.15255v4), [pdf](http://arxiv.org/pdf/2305.15255v4.pdf), cication: [**2**](https://scholar.google.com/scholar?cites=2239314690427158927&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Eliya Nachmani, Alon Levkovitch, Roy Hirsch, Julian Salazar, Chulayuth Asawaroengchai, Soroosh Mariooryad, Ehud Rivlin, RJ Skerry-Ryan, Michelle Tadmor Ramanovich* Â· ([research](https://research.google/blog/spoken-question-answering-and-speech-continuation-using-a-spectrogram-powered-llm/)) Â· ([LLAMA1-Test-Set](https://github.com/google-research-datasets/LLAMA1-Test-Set) - google-research-datasets) ![Star](https://img.shields.io/github/stars/google-research-datasets/LLAMA1-Test-Set.svg?style=social&label=Star) Â· ([michelleramanovich.github](https://michelleramanovich.github.io/spectron/spectron))
- **Text-Free Prosody-Aware Generative Spoken Language Modeling**, `arXiv, 2109.03264`, [arxiv](http://arxiv.org/abs/2109.03264v2), [pdf](http://arxiv.org/pdf/2109.03264v2.pdf), cication: [**86**](https://scholar.google.com/scholar?cites=6860409912627023988&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Eugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi, Jade Copet, Kushal Lakhotia, Tu-Anh Nguyen, Morgane RiviÃ¨re, Abdelrahman Mohamed, Emmanuel Dupoux*

## Projects
- [**flow_mirror**](https://github.com/jingzhunxue/flow_mirror/tree/main) - jingzhunxue ![Star](https://img.shields.io/github/stars/jingzhunxue/flow_mirror.svg?style=social&label=Star)

	 *flow mirror models from JZX AI Labs*
- [Llama3 just got ears - Homebrew](https://homebrew.ltd/blog/llama3-just-got-ears)

	 Â· ([huggingface](https://huggingface.co/homebrewltd/Llama3.1-s-instruct-2024-08-19-epoch-3))
- [**speech-to-speech**](https://github.com/huggingface/speech-to-speech) - huggingface ![Star](https://img.shields.io/github/stars/huggingface/speech-to-speech.svg?style=social&label=Star)
- [SpeechGPT2: End-to-End Human-Like Spoken Chatbot](https://0nutation.github.io/SpeechGPT2.github.io/)
- [The Textless NLP project](https://speechbot.github.io/)
- [**Desktop_BUD-E**](https://github.com/christophschuhmann/Desktop_BUD-E/tree/main) - christophschuhmann ![Star](https://img.shields.io/github/stars/christophschuhmann/Desktop_BUD-E.svg?style=social&label=Star)

	 *This is a voice assitant to run on you laptop*
- [Call to Build Open Multi-Modal Models for Personal Assistants | LAION](https://laion.ai/notes/open-gpt-4-o/)
- [Building GPT2o â€” Part 1 : Audio. code, pretrained model, colab notebook | by Srinivas Billa | Jun, 2024 | Medium](https://medium.com/@nivibilla/building-gpt2o-part-1-audio-65b66e193784)

	 Â· ([build-nanogpt](https://github.com/nivibilla/build-nanogpt/tree/audio) - nivibilla) ![Star](https://img.shields.io/github/stars/nivibilla/build-nanogpt.svg?style=social&label=Star)
- [**natural_voice_assistant**](https://github.com/LAION-AI/natural_voice_assistant) - LAION-AI ![Star](https://img.shields.io/github/stars/LAION-AI/natural_voice_assistant.svg?style=social&label=Star)

	 Â· ([laion](https://laion.ai/blog/bud-e/))
- [**WhisperFusion**](https://github.com/collabora/WhisperFusion) - collabora ![Star](https://img.shields.io/github/stars/collabora/WhisperFusion.svg?style=social&label=Star)

	 *WhisperFusion builds upon the capabilities of WhisperLive and WhisperSpeech to provide a seamless conversations with an AI.*
- [**natural_voice_assistant**](https://github.com/LAION-AI/natural_voice_assistant) - LAION-AI ![Star](https://img.shields.io/github/stars/LAION-AI/natural_voice_assistant.svg?style=social&label=Star)
- [**pipecat**](https://github.com/pipecat-ai/pipecat) - pipecat-ai ![Star](https://img.shields.io/github/stars/pipecat-ai/pipecat.svg?style=social&label=Star)

	 *Open Source framework for voice and multimodal conversational AI*
- [Audio Text GPT - Google Docs](https://docs.google.com/document/d/1nfngl0lv3XAWLQg5z-euYcE6NGGaoH7BdyoujVf7tEc/edit)
- [**gazelle**](https://github.com/tincans-ai/gazelle/tree/main?tab=readme-ov-file) - tincans-ai ![Star](https://img.shields.io/github/stars/tincans-ai/gazelle.svg?style=social&label=Star)

	 *Joint speech-language model - respond directly to audio!* Â· ([tincans](https://tincans.ai/slm)) Â· ([x](https://x.com/hingeloss/status/1765440068452331898)) Â· ([x](https://x.com/hingeloss/status/1770157984745656743))
## Acoustic echo cancellation
- [**NKF-AEC**](https://github.com/fjiang9/NKF-AEC?tab=readme-ov-file) - fjiang9 ![Star](https://img.shields.io/github/stars/fjiang9/NKF-AEC.svg?style=social&label=Star)

	 *Acoustic Echo Cancellation with Nerual Kalman Filtering* Â· ([fjiang9.github](https://fjiang9.github.io/NKF-AEC/))
- [**DTLN-aec**](https://github.com/breizhn/DTLN-aec) - breizhn ![Star](https://img.shields.io/github/stars/breizhn/DTLN-aec.svg?style=social&label=Star)

	 *This Repostory contains the pretrained DTLN-aec model for real-time acoustic echo cancellation.*
- [**TSPNN**](https://github.com/enhancer12/TSPNN) - enhancer12 ![Star](https://img.shields.io/github/stars/enhancer12/TSPNN.svg?style=social&label=Star)

	 *Two-stage progressive neural network for acoustic echo cancellation*

## End of Speech 
- **Beyond the Turn-Based Game: Enabling Real-Time Conversations with Duplex
  Models**, `arXiv, 2406.15718`, [arxiv](http://arxiv.org/abs/2406.15718v1), [pdf](http://arxiv.org/pdf/2406.15718v1.pdf), cication: [**-1**](None)

	 *Xinrong Zhang, Yingfa Chen, Shengding Hu, Xu Han, Zihang Xu, Yuanwei Xu, Weilin Zhao, Maosong Sun, Zhiyuan Liu*
- **A Full-duplex Speech Dialogue Scheme Based On Large Language Models**, `arXiv, 2405.19487`, [arxiv](http://arxiv.org/abs/2405.19487v1), [pdf](http://arxiv.org/pdf/2405.19487v1.pdf), cication: [**-1**](None)

	 *Peng Wang, Songshuo Lu, Yaohua Tang, Sijie Yan, Yuanjun Xiong, Wei Xia*
- [**predictivechat**](https://github.com/yoheinakajima/predictivechat?tab=readme-ov-file) - yoheinakajima ![Star](https://img.shields.io/github/stars/yoheinakajima/predictivechat.svg?style=social&label=Star)

	 *Demo of AI chatbot that predicts user message to generate response quickly.*
- **TurnGPT: a Transformer-based Language Model for Predicting Turn-taking
  in Spoken Dialog**, `arXiv, 2010.10874`, [arxiv](http://arxiv.org/abs/2010.10874v1), [pdf](http://arxiv.org/pdf/2010.10874v1.pdf), cication: [**45**](https://scholar.google.com/scholar?cites=4498969182199038561&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Erik Ekstedt, Gabriel Skantze*


## Tutorials
- [Moshi (@kyutai_labs) is an unpolished e2e *full-duplex* model](https://x.com/JulianSlzr/status/1810303916686577858)
- [SNAC with flattening & reconstruction - YouTube](https://www.youtube.com/watch?v=NwZufAJxmMA&ab_channel=DemocratizingAI)

## Datasets
- https://huggingface.co/datasets/TwoAbove/the-project-gutenberg-open-audiobook-collection
- https://huggingface.co/datasets/EQ4You/Emotional_Speech ( gender labels are broken â†’ â€œMaleâ€ and â€œfemaleâ€ need to be removed from all the captions, man and Woman need to be replaced with Person )
- https://huggingface.co/datasets/speechcolab/gigaspeech
- https://huggingface.co/datasets/parler-tts/mls-eng-10k-tags_tagged_10k_generated/viewer/default/train
- https://huggingface.co/datasets/krishnakalyan3/emo_300k 
- https://huggingface.co/datasets/mozilla-foundation/common_voice_17_0
- https://huggingface.co/datasets/facebook/multilingual_librispeech 
- https://huggingface.co/datasets/librispeech_asr 
- https://huggingface.co/collections/marianna13/laion-audio-630k-65aff8bbaa335c2842f3a730 
- https://huggingface.co/datasets/blanchon/udio_dataset 


- https://huggingface.co/datasets/ChristophSchuhmann/yt-urls-for-emotional-tts
- https://huggingface.co/datasets/ChristophSchuhmann/docu-clips
- https://huggingface.co/datasets/ChristophSchuhmann/movie-clips

- [collabora/librilight-processed-webdataset at main](https://huggingface.co/datasets/collabora/librilight-processed-webdataset/tree/main)
- [LAION-Audio-630k - a marianna13 Collection](https://huggingface.co/collections/marianna13/laion-audio-630k-65aff8bbaa335c2842f3a730)

### LLM
- [**SlimOrca-Dedup**](https://huggingface.co/datasets/Open-Orca/SlimOrca-Dedup?row=0) - Open-Orca ðŸ¤—

## Evaluation
- **SD-Eval: A Benchmark Dataset for Spoken Dialogue Understanding Beyond
  Words**, `arXiv, 2406.13340`, [arxiv](http://arxiv.org/abs/2406.13340v1), [pdf](http://arxiv.org/pdf/2406.13340v1.pdf), cication: [**-1**](None)

	 *Junyi Ao, Yuancheng Wang, Xiaohai Tian, Dekun Chen, Jun Zhang, Lu Lu, Yuxuan Wang, Haizhou Li, Zhizheng Wu* Â· ([SD-Eval](https://github.com/amphionspace/SD-Eval) - amphionspace) ![Star](https://img.shields.io/github/stars/amphionspace/SD-Eval.svg?style=social&label=Star)
## Products
- [Sindarin â€” Build your own Conversational Speech AI.](https://www.sindarin.tech/)
- [Introducing Deepgram Aura: Lightning Fast Text-to-Speech for Voice AI Agents | Deepgram](https://deepgram.com/learn/aura-text-to-speech-tts-api-voice-ai-agents-launch)

## Demos
- [ChatGPT heavy breathing and shouting : r/singularity](https://www.reddit.com/r/singularity/comments/1eskpsb/chatgpt_heavy_breathing_and_shouting/?utm_source=ainews&utm_medium=email&utm_campaign=ainews-not-much-happened-today-5446)