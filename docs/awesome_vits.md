# Awesome VITS

- [Awesome VITS](#awesome-vits)
  - [Projects](#projects)
  - [Papers](#papers)
  - [Reference](#reference)


## Projects
- [**ar-vits**](https://github.com/innnky/ar-vits?tab=readme-ov-file) - innnky ![Star](https://img.shields.io/github/stars/innnky/ar-vits.svg?style=social&label=Star)

	 *text to speech using autoregressive transformer and VITS*

## Papers
- **VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech
  with Adversarial Learning and Architecture Design**, `arXiv, 2307.16430`, [arxiv](http://arxiv.org/abs/2307.16430v1), [pdf](http://arxiv.org/pdf/2307.16430v1.pdf), cication: [**-1**](None)

	 *Jungil Kong, Jihoon Park, Beomjeong Kim, Jeongmin Kim, Dohee Kong, Sangjin Kim*

    - [GitHub - p0p4k/vits2\_pytorch: unofficial vits2-TTS implementation in pytorch](https://github.com/p0p4k/vits2_pytorch)
    - [GitHub - daniilrobnikov/vits2: VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design](https://github.com/daniilrobnikov/vits2)
    - [Audio samples from "VITS2: Improving Quality and Efficiency of Single Stage Text to Speech with Adversarial Learning and Architecture Design"](https://vits-2.github.io/demo/)
    - [GitHub - fishaudio/Bert-VITS2: vits2 backbone with bert](https://github.com/fishaudio/Bert-VITS2)

- **NaturalSpeech: End-to-End Text to Speech Synthesis with Human-Level
  Quality**, `arXiv, 2205.04421`, [arxiv](http://arxiv.org/abs/2205.04421v2), [pdf](http://arxiv.org/pdf/2205.04421v2.pdf), cication: [**-1**](None)

	 *Xu Tan, Jiawei Chen, Haohe Liu, Jian Cong, Chen Zhang, Yanqing Liu, Xi Wang, Yichong Leng, Yuanhao Yi, Lei He*
    - [GitHub - heatz123/naturalspeech: A fully working pytorch implementation of NaturalSpeech (Tan et al., 2022)](https://github.com/heatz123/naturalspeech)
    - [NaturalSpeech: End-to-End Text to Speech Synthesis with Human-Level Quality - Speech Research](https://speechresearch.github.io/naturalspeech/)

- **Conditional Variational Autoencoder with Adversarial Learning for
  End-to-End Text-to-Speech**, `arXiv, 2106.06103`, [arxiv](http://arxiv.org/abs/2106.06103v1), [pdf](http://arxiv.org/pdf/2106.06103v1.pdf), cication: [**-1**](None)

	 *Jaehyeon Kim, Jungil Kong, Juhee Son*
    - VITS: [60ã€åŸºäºcVAE+Flow+GANçš„æ•ˆæœæœ€å¥½è¯­éŸ³åˆæˆVITSæ¨¡å‹è®ºæ–‡ç²¾è®²\_å“”å“©å“”å“©\_bilibili](https://www.bilibili.com/video/BV1wU4y1q7po/?spm_id_from=333.337.search-card.all.click&vd_source=1453a06a1e0b377f5c40946333b4423a)
    - [Audio Samples from "Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech"](https://jaywalnut310.github.io/vits-demo/index.html)
    - [GitHub - jaywalnut310/vits: VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech](https://github.com/jaywalnut310/vits)

## Reference
- [**finetune-hf-vits**](https://github.com/ylacombe/finetune-hf-vits) - ylacombe ![Star](https://img.shields.io/github/stars/ylacombe/finetune-hf-vits.svg?style=social&label=Star)

	 *Finetune VITS and MMS using HuggingFace's tools*
- [**explore-vits**](https://huggingface.co/spaces/hf-audio/explore-vits) - hf-audio ğŸ¤—
- [ä¸¾ä¸–æ— åŒè¯­éŸ³åˆæˆç³»ç»Ÿ VITS å‘å±•å†ç¨‹ï¼ˆ2023.03.31 SNACï¼‰ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/474601997)